{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c48eae2-5997-43f9-af34-9b5bb24fb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518087f-2cb5-4289-8743-dbfc32f7083b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Data Mining\n",
    "\n",
    "Data mining is the process of discovering meaningful patterns, trends, and relationships in large datasets using statistical, machine learning, and database management techniques. It goes beyond simple data analysis by automatically extracting hidden knowledge that can support decision-making, and helps us understand complex phenomena. Common applications include customer behavior analysis, fraud detection, medical diagnosis, or market trend prediction. By turning raw data into actionable insights, data mining serves as a critical tool in today’s data-driven world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2052d-9b87-400d-a1d3-0075be81d982",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Data\n",
    "\n",
    "The most convenient way to think of the datasets that the majority\n",
    "of data mining algorithms operate upon is the tabular view. In this\n",
    "analogy the problem at hand can be treated as (a potentially gigantic)\n",
    "spreadsheet with several rows – corresponding to data objects – and\n",
    "columns, each of which includes observed attributes with respect the\n",
    "different aspects of these data objects.\n",
    "\n",
    "Another important aspect of the datasets we work with is the\n",
    "measurement scale of the individual columns in the data matrix\n",
    "(each corresponding to a random variable). A concise summary of\n",
    "the different measurement scales and some of the most prototypical\n",
    "statistics which can be calculated for them:\n",
    "\n",
    "| Type of attribute | Description | Examples | Statistics |\n",
    "|-------------------|-------------|----------|------------|\n",
    "| **Categorical**   |             |          |            |\n",
    "| Nominal           | Variables can be checked for equality only; | names of cities, hair color | mode, entropy, correlation, χ²-test |\n",
    "| Ordinal           |  `>` relation can be interpreted among variables; | grades {fail, pass, excellent} | median, percentiles |\n",
    "| **Numerical**     |             |          |            |\n",
    "| Interval          | The difference of two variables can be formed and interpreted | shoe sizes, dates, °C | mean, deviation, significance (e.g., F-, t- tests) |\n",
    "| Ratio             | Ratios can be formed from values of the variables of this kind | age, length, temperature in Kelvin | percent, geometric/harmonic mean, variation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeadae99-d7fc-47d7-b3fd-6aa187b00c08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's load the [Bike Rental Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/bike-sharing-dataset). A slightly modified version that we are going to use can be found at `/data/rental.csv`.\n",
    "\n",
    "Dataset features:\n",
    "- `season`: Season\n",
    "- `yr`: Year\n",
    "- `mnth`: Month\n",
    "- `holiday`: Indicator whether the day was a holiday or not.\n",
    "- `weekday`: Day of the week.\n",
    "- `workingday`: Indicator whether the day was a working day or weekend.\n",
    "- `weathersit`: The weather situation on that day. One of:\n",
    "  - 1: clear, few clouds, partly cloudy, cloudy\n",
    "  - 2: mist + clouds, mist + broken clouds, mist + few clouds, mist\n",
    "  - 3: light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds\n",
    "  - 4: heavy rain + ice pallets + thunderstorm + mist, snow + mist\n",
    "- `temp`: Temperature in degrees Celsius.\n",
    "- `atemp`: Felt temperature in Celsius.\n",
    "- `hum`: Relative humidity in percent (0 to 100).\n",
    "- `windspeed`: Wind speed in km per hour.\n",
    "- `cnt`: Count of bicycles including both casual and registered users. The count is used as the target in the regression task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a45931b-98b8-4c3f-b3f2-726aa5feefae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>january</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.175849</td>\n",
       "      <td>39.999250</td>\n",
       "      <td>80.5833</td>\n",
       "      <td>10.749882</td>\n",
       "      <td>985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>january</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.083466</td>\n",
       "      <td>39.346774</td>\n",
       "      <td>69.6087</td>\n",
       "      <td>16.652113</td>\n",
       "      <td>801.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>january</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.229108</td>\n",
       "      <td>28.500730</td>\n",
       "      <td>43.7273</td>\n",
       "      <td>16.636703</td>\n",
       "      <td>1349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>january</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>30.000052</td>\n",
       "      <td>59.0435</td>\n",
       "      <td>10.739832</td>\n",
       "      <td>1562.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>january</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.666979</td>\n",
       "      <td>31.131820</td>\n",
       "      <td>43.6957</td>\n",
       "      <td>12.522300</td>\n",
       "      <td>1600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>december</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.945849</td>\n",
       "      <td>30.958372</td>\n",
       "      <td>65.2917</td>\n",
       "      <td>23.458911</td>\n",
       "      <td>2114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>december</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.906651</td>\n",
       "      <td>32.833036</td>\n",
       "      <td>59.0000</td>\n",
       "      <td>10.416557</td>\n",
       "      <td>3095.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>december</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.906651</td>\n",
       "      <td>31.998400</td>\n",
       "      <td>75.2917</td>\n",
       "      <td>8.333661</td>\n",
       "      <td>1341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>december</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.024151</td>\n",
       "      <td>31.292200</td>\n",
       "      <td>48.3333</td>\n",
       "      <td>23.500518</td>\n",
       "      <td>1796.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>december</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.144151</td>\n",
       "      <td>30.750142</td>\n",
       "      <td>57.7500</td>\n",
       "      <td>10.374682</td>\n",
       "      <td>2729.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>731 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     season    yr      mnth  holiday  weekday  workingday  weathersit  \\\n",
       "0    spring  2011   january      0.0      6.0         0.0         2.0   \n",
       "1    spring  2011   january      0.0      0.0         0.0         2.0   \n",
       "2    spring  2011   january      0.0      1.0         1.0         1.0   \n",
       "3    spring  2011   january      0.0      2.0         1.0         1.0   \n",
       "4    spring  2011   january      0.0      3.0         1.0         1.0   \n",
       "..      ...   ...       ...      ...      ...         ...         ...   \n",
       "726  spring  2012  december      0.0      4.0         1.0         2.0   \n",
       "727  spring  2012  december      0.0      5.0         1.0         2.0   \n",
       "728  spring  2012  december      0.0      6.0         0.0         2.0   \n",
       "729  spring  2012  december      0.0      0.0         0.0         1.0   \n",
       "730  spring  2012  december      0.0      1.0         1.0         2.0   \n",
       "\n",
       "          temp      atemp      hum  windspeed     cnt  \n",
       "0    24.175849  39.999250  80.5833  10.749882   985.0  \n",
       "1    25.083466  39.346774  69.6087  16.652113   801.0  \n",
       "2    17.229108  28.500730  43.7273  16.636703  1349.0  \n",
       "3    17.400000  30.000052  59.0435  10.739832  1562.0  \n",
       "4    18.666979  31.131820  43.6957  12.522300  1600.0  \n",
       "..         ...        ...      ...        ...     ...  \n",
       "726  19.945849  30.958372  65.2917  23.458911  2114.0  \n",
       "727  19.906651  32.833036  59.0000  10.416557  3095.0  \n",
       "728  19.906651  31.998400  75.2917   8.333661  1341.0  \n",
       "729  20.024151  31.292200  48.3333  23.500518  1796.0  \n",
       "730  18.144151  30.750142  57.7500  10.374682  2729.0  \n",
       "\n",
       "[731 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "df = pd.read_csv(\"https://github.com/ficstamas/data-mining/raw/b76d5b7913c446878fa47de8861c83e26780828f/data/rental.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad4fd7c7-16ea-4332-878c-128af3ca8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the train-test splits and separate the target variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_X, train_y = train[train.columns.difference([\"cnt\"])], train[[\"cnt\"]]\n",
    "test_X, test_y = test[test.columns.difference([\"cnt\"])], test[[\"cnt\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7104b8-2a68-4d9e-af8b-5086a89b12dc",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17e772a4-8830-447a-895f-8a90d38d19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a histogram visualize the distribution of numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f921636a-c6ac-4115-98ee-5688911f4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the joint behaviour of two variables using scatter plot\n",
    "# extra: include the histogram of the the point next to the x and y axes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd63698b-d2a4-46ac-b39e-3f1047d987a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the distribution of windspeed for each season (using box or violin plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b82c0710-7c4f-4123-98fd-8b24f6a75116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualize any interesing asapect of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130eafb-cb8a-4788-901f-0af9027a3ccd",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2af01340-40d6-4295-946b-65178e3ef64a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model fitting and evaluation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def fit_and_eval(trainX, trainY, testX, testY):\n",
    "    model = LinearRegression()\n",
    "    model.fit(trainX, trainY)\n",
    "    \n",
    "    predictions = model.predict(testX)\n",
    "    return r2_score(testY, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7c44d-a234-4a95-8b67-f66baa8baf89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "Data preprocessing is a crucial step in any data analysis or machine learning workflow because raw data is often incomplete, inconsistent, noisy, or in a format unsuitable for modeling. Preprocessing transforms this raw data into a clean, well-structured form, ensuring that the analysis or model can extract meaningful patterns rather than being misled by errors or irrelevant information. This means that we perform some transformation over the data matrix, either in a column or a row-oriented manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb46f07-93c3-4319-8bc4-72c322015510",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Basic Concepts\n",
    "\n",
    "The **mean (or expected) value** can be calculated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ec5d5-e0d0-4046-b941-7fad5ec5a0b0",
   "metadata": {},
   "source": [
    "$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{x}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c249953-5604-4065-ac1d-717d9d62a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of a random vector without using any builtin or numpy functions\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(50)\n",
    "\n",
    "# code here, X_mean should contain the final result\n",
    "X_mean = None\n",
    "\n",
    "assert np.allclose(np.mean(X), X_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df692fed-718a-439a-982c-80a539f415dd",
   "metadata": {},
   "source": [
    "The **variance** measures how much a set of **values deviates from their mean**. A small variance indicates that the data points are close to the mean, while a large variance indicates more spread. It can be seen as the average distance from the mean.\n",
    "$$\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar{x})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac94a89-f527-45fa-90d5-7af54e9731af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standarde deviation of a random vector without using any builtin or numpy functions\n",
    "# you can reuse code from previous tasks\n",
    "np.random.seed(0)\n",
    "X = np.random.multivariate_normal([0], [[3]], 50)\n",
    "\n",
    "# code here X_var should contain the final result\n",
    "X_var = None\n",
    "\n",
    "assert np.allclose(np.std(X), X_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437255cb-e71a-49a0-86d5-2681deb83648",
   "metadata": {},
   "source": [
    "The **standard deviation** is the squere root of the variance. It can be useful if you want measure the spread withing the same units as your input.\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9355b09-6d4e-40dc-932b-190eb6c8937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standarde deviation of a random vector without using any builtin or numpy functions\n",
    "# you can reuse code from previous tasks\n",
    "np.random.seed(0)\n",
    "X = np.random.multivariate_normal([0], [[3]], 50)\n",
    "\n",
    "# code here X_std should contain the final result\n",
    "X_std = None\n",
    "\n",
    "assert np.allclose(np.std(X), X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13fe969-8d8b-4b8e-9687-22457ddf3dfe",
   "metadata": {},
   "source": [
    "The covariance matrix generalizes the concept of variance to multiple dimensions, measuring how pairs of features vary together.\n",
    "$$\\Sigma = \\mathrm{Cov}(\\mathbf{X}) = \\frac{1}{n} (\\mathbf{X} - \\bar{\\mathbf{X}})^\\top (\\mathbf{X} - \\bar{\\mathbf{X}})$$\n",
    "\n",
    "Each element ($\\Sigma_{ij}$) represents the covariance between features $i$ and $j$, while the diagonal entries correspond to the variances of individual features. A positive covariance indicates that the features increase together, while a negative covariance indicates an inverse relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb56f9-d1ef-4710-8103-f3614cd88190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the covariance matrix without using any builtin functions\n",
    "# you can reuse code from previous tasks\n",
    "np.random.seed(0)\n",
    "X = np.random.multivariate_normal([0, 0, 0], [[1, 0.3, 0.1], [0.3, 1, -0.5], [0.1, -0.5, 1]], 50)\n",
    "\n",
    "X_cov = None\n",
    "\n",
    "assert np.allclose(X_cov, np.cov(X.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266dab23-5390-49d0-89e8-4fdef64aa3e4",
   "metadata": {},
   "source": [
    "### Correlation and Causuality\n",
    "\n",
    "**Correlation refers to a statistical relationship between two variables** - when changes in one variable are associated with changes in another. For example, ice cream sales and beach attendance often rise together, showing a positive correlation. **Causality, on the other hand, means that one event directly influences or produces another.** If A causes B, then changing A will lead to a predictable change in B. While correlation can hint at possible causal links, it does not prove them.\n",
    "\n",
    "The key difference is that correlation simply describes a relationship, while causality explains the underlying mechanism of that relationship. Many correlated events share a common cause or are influenced by other variables (confounders). For instance, both ice cream sales and drowning incidents increase in summer, but the cause is warmer weather - not ice cream itself.\n",
    "\n",
    "**A common misconception is assuming that \"correlation implies causation\".** This error, sometimes called the [post hoc fallacy](https://en.wikipedia.org/wiki/Post_hoc_ergo_propter_hoc), can lead to flawed conclusions in research, business, and policy-making. Proper causal inference requires careful experimental design, statistical controls, or methods like randomized controlled trials, not just observational data. In short: correlation can point you toward possible causes, but causality must be proven through deeper investigation.\n",
    "\n",
    "[It is not that hard to find missleading examples!](https://www.tylervigen.com/spurious-correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d19984-8186-41b9-9765-d20a44db9c21",
   "metadata": {},
   "source": [
    "The linear correlation coefficient (Pearson correlation coefficient) can be calculated as:\n",
    "$$\\rho_{x,y} = \\frac{\\mathrm{Cov}(x, y)}{\\sigma_x \\sigma_y} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b2932c59-fedc-4d27-9685-4410995f2272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation matrix without using any builtin functions\n",
    "# no bias correction\n",
    "# you can reuse code from previous tasks\n",
    "np.random.seed(0)\n",
    "X = np.random.multivariate_normal([0, 0, 0], [[1, 0.3, 0.1], [0.3, 1, -0.5], [0.1, -0.5, 1]], 50)\n",
    "\n",
    "X_cor = None\n",
    "\n",
    "assert np.allclose(X_cor, np.corrcoef(X.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8a7bf-3435-4f76-865e-d3ca298b2b0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform correlation analysis on the features\n",
    "# from scipy.stats import pearsonr\n",
    "# np.corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc4240-54e9-43f2-9dbb-c2fec70d300e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Mutual Information\n",
    "\n",
    "Mutual information is a fundamental concept in information theory that quantifies the amount of information shared between two random variables. Unlike correlation, which only captures linear relationships, mutual information measures all types of dependencies, whether linear or nonlinear. It essentially tells us how much knowing the value of one variable reduces the uncertainty (entropy) about the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e76a50-b341-4f90-984a-81737c19224b",
   "metadata": {},
   "source": [
    "Entropy can be seen as a measurement that quantifies the amount of uncertainty in a feature, which can be formulated as:\n",
    "$$H(X) = - \\sum_{x \\in \\mathcal{X}} p(x) \\log p(x).$$\n",
    "Mutual information can be expressed between two variable $X$ and $Y$ as:\n",
    "$$I(X;Y) = \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)},$$\n",
    "which can be also expressed by entropy:\n",
    "$$I(X;Y) = H(X) + H(Y) - H(X,Y).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5bf00f-a046-444e-95b7-18e4762b0e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Shannon entropy of numerical features\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162376dd-c74b-485b-9d0b-1d3d68588512",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the mutual information between `season` and `weathersit`\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d61094-aecf-4b8a-9117-8023ac35a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By relying on mutual information, try to discratize the `windspeed` feature into 4 differen categories ('low', 'medium', 'high', 'very high')\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142df19-1912-46a1-9bd6-642fb3c6ce56",
   "metadata": {},
   "source": [
    "###  Eigenvalues and Eigenvectors\n",
    "\n",
    "Eigenvalues and eigenvectors are central concepts in linear algebra that provide deep insights into the behavior of linear transformations represented by matrices. Given a square matrix $A$, an eigenvector $v$ is a non-zero vector that, when multiplied by $A$, only changes in magnitude and not in direction. The factor by which it is scaled is called the eigenvalue $\\lambda$, satisfying the equation $Av=\\lambda v$. Eigenvalues and eigenvectors reveal fundamental properties of a matrix, such as its invertibility, stability, and the principal directions along which transformations act."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d8d2930-b4d2-4b28-a7ea-2a4b28813978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea1385311124ad587dbb34d7bd377aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='a11', max=3.0, min=-3.0), FloatSlider(value=0.5, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    a11=widgets.FloatSlider(min=-3, max=3, step=0.1, value=1),\n",
    "    a12=widgets.FloatSlider(min=0, max=3, step=0.1, value=0.5),\n",
    "    a21=widgets.FloatSlider(min=0, max=3, step=0.1, value=0.5),\n",
    "    a22=widgets.FloatSlider(min=-3, max=3, step=0.1, value=1),\n",
    ")\n",
    "def plot_eigen(a11=2.0, a12=0.0, a21=0.0, a22=1.0):\n",
    "    A = np.array([[a11, a12],\n",
    "                  [a21, a22]])\n",
    "    \n",
    "    # Create a unit circle\n",
    "    theta = np.linspace(0, 2*np.pi, 200)\n",
    "    circle = np.vstack([np.cos(theta), np.sin(theta)])\n",
    "\n",
    "    # Transform circle\n",
    "    ellipse = A @ circle\n",
    "    \n",
    "    # Try computing eigenvalues/vectors\n",
    "    try:\n",
    "        vals, vecs = np.linalg.eig(A)\n",
    "    except np.linalg.LinAlgError:\n",
    "        vals, vecs = [], []\n",
    "\n",
    "    print(vecs)\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    \n",
    "    # Original circle\n",
    "    plt.plot(circle[0], circle[1], 'k--', alpha=0.5, label=\"Unit circle\")\n",
    "    \n",
    "    # Transformed ellipse\n",
    "    plt.plot(ellipse[0], ellipse[1], 'b-', label=\"Transformed circle\")\n",
    "    \n",
    "    # Plot eigenvectors\n",
    "    for val, vec in zip(vals, vecs.T):\n",
    "        if np.iscomplex(val):  # skip complex eigenvalues\n",
    "            continue\n",
    "        vec = vec / np.linalg.norm(vec)  # normalize\n",
    "        plt.arrow(0, 0, val*vec[0], val*vec[1],\n",
    "                  head_width=0.1, color='r', alpha=0.8,\n",
    "                  length_includes_head=True)\n",
    "        plt.text(val*vec[0]*1.1, val*vec[1]*1.1, f\"λ={val:.2f}\", color='r')\n",
    "    \n",
    "    plt.axhline(0, color='gray', lw=0.5)\n",
    "    plt.axvline(0, color='gray', lw=0.5)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.legend()\n",
    "    plt.title(f\"Matrix A = [[{a11:.1f}, {a12:.1f}], [{a21:.1f}, {a22:.1f}]]\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7342229-4f64-4acf-8d41-df76fb5b29b4",
   "metadata": {},
   "source": [
    "## Categorical data\n",
    "\n",
    "Categorical data transformation is a critical step in data preprocessing, as it converts nominal variables into numerical formats that machine learning algorithms can interpret. Many statistical and predictive models rely on mathematical operations, which cannot be directly applied to raw categorical values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7a6ed-1db7-409e-8474-3abd18ecbac4",
   "metadata": {},
   "source": [
    "### Numeric mapping\n",
    "\n",
    "Numeric mapping ensures that each categorical value will have a numerical value, for example instead representing `seasons` as strings, we remap each value in the following manner `{'spring': 0, 'summer': 1, 'fall': 2, 'winter': 3}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16ce7b70-c0c8-42e9-9ee6-94cc932cfa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets transform each categorical feature to their numerical representation\n",
    "# \n",
    "def _transform_categorical_to_numerical(row):\n",
    "    \"\"\"\n",
    "    row: A row of the dataset. You can access a column by `row.column_name` or row[\"column_name\"]\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    return row\n",
    "\n",
    "\n",
    "numeric_mapping_train_X = train_X.apply(_transform_categorical_to_numerical, axis=1)\n",
    "numeric_mapping_test_X = test_X.apply(_transform_categorical_to_numerical, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09b7556a-f506-4006-b506-06e9afc0f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "# fit_and_eval(numeric_mapping_train_X, trainY, numeric_mapping_test_X, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32277d64-1897-40ab-af61-5b39a198a68d",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "One-hot encoding is a common method for representing categorical variables as binary vectors, where each category is assigned its own column and marked with a `1` for presence and `0` for absence. This approach avoids imposing any false ordinal relationship between categories, making it ideal for nominal data. While it preserves category distinctions, it can significantly increase dimensionality, especially for features with many unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25c6b1b0-18f4-415a-afe2-20b4c198261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets transform each categorical feature to their one-hot encoded representation\n",
    "# \n",
    "\n",
    "\n",
    "one_hot_mapping_train_X = None\n",
    "one_hot_mapping_test_X = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33dba0cb-2da8-464c-afc4-dff6552ffe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "# fit_and_eval(one_hot_mapping_train_X, trainY, one_hot_mapping_test_X, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6220961c-0ad0-4a01-9b1e-b82006def6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to remove dummy variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0834096-314a-4847-8e97-a8475c830cbe",
   "metadata": {},
   "source": [
    "## Numerical data\n",
    "\n",
    "Numerical data transformation involves modifying quantitative variables to improve model performance, interpretability, or compliance with algorithmic assumptions. Common techniques include normalization, standardization, scaling, and non-linear transformations, which help adjust for differences in magnitude, distribution skewness, or outliers. By transforming numerical data appropriately, we can enhance learning efficiency, reduce bias from extreme values, and ensure that each feature contributes proportionately to the model. In short, we can help the model fit better by scaling the numerical features to similar scales (i.e. `windspeed` in km/h scales differently than `temp` in celsius)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2fef35-e74a-4e13-b227-d13f920bddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for plotting\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "\n",
    "def plot_enclosing_ellipse(A, ax, std_levels=(1.0, 2.0, 3.0), styles=('-', '--', ':'), facecolor='none', **kwargs):\n",
    "    x, y = A[:, 0], A[:, 1]\n",
    "    cov = np.cov(x, y)\n",
    "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensional dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    for i, sigma in enumerate(std_levels):\n",
    "        # facecolor=facecolor\n",
    "        ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2, linestyle=styles[i], **kwargs)\n",
    "    \n",
    "        # Calculating the standard deviation of x from\n",
    "        # the squareroot of the variance and multiplying\n",
    "        # with the given number of standard deviations.\n",
    "        scale_x = np.sqrt(cov[0, 0]) * sigma\n",
    "        mean_x = np.mean(x)\n",
    "    \n",
    "        # calculating the standard deviation of y ...\n",
    "        scale_y = np.sqrt(cov[1, 1]) * sigma\n",
    "        mean_y = np.mean(y)\n",
    "    \n",
    "        transf = transforms.Affine2D() \\\n",
    "            .rotate_deg(45) \\\n",
    "            .scale(scale_x, scale_y) \\\n",
    "            .translate(mean_x, mean_y)\n",
    "    \n",
    "        ellipse.set_transform(transf + ax.transData)\n",
    "        ax.add_patch(ellipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e71ebca-17ba-452c-a68f-3fd048ea0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformation(x, x_altered, altered_label, axs):  \n",
    "    center = np.mean(x, axis=0)  \n",
    "    axs.scatter(x[:, 0], x[:, 1], color=\"red\", label=\"Original Data\")\n",
    "    axs.scatter(center[0], center[1], marker=\"x\", color=\"blue\", label=\"Mean\")\n",
    "    \n",
    "    plot_enclosing_ellipse(x, axs, fill=False, color='red', linewidth=1)\n",
    "    \n",
    "\n",
    "    _m_centered = np.mean(x_altered, axis=0)\n",
    "    axs.scatter(x_altered[:, 0], x_altered[:, 1], color=\"yellow\", label=altered_label)\n",
    "    axs.scatter(_m_centered[0], _m_centered[1], marker=\"x\", color=\"blue\")\n",
    "    plot_enclosing_ellipse(x_altered, axs, fill=False, color='yellow', linewidth=1)\n",
    "    \n",
    "    axs.annotate(\"\", xytext=(center[0], center[1]), xy=(_m_centered[0], _m_centered[1]), arrowprops=dict(arrowstyle=\"->\", color=\"red\"))\n",
    "    xlim = axs.get_xlim()\n",
    "    ylim = axs.get_ylim()\n",
    "    axs.vlines(0, -1000, 1000, color=\"black\", lw=1)\n",
    "    axs.hlines(0, -1000, 1000, color=\"black\", lw=1)\n",
    "    axs.set_xlim(*xlim)\n",
    "    axs.set_ylim(*ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b71a86-fbb3-44ed-8fb9-2f2f1d641050",
   "metadata": {},
   "source": [
    "### Mean centering\n",
    "\n",
    "Mean centering is a technique in which the average value of a variable is subtracted from each data point, resulting in a new variable with a mean of zero.\n",
    "\n",
    "$$x' = x - \\bar{x}, \\quad \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "79452d73-895e-40dc-9ffa-49ac8ca08561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385ee938887244ebb14f6a7c721f554d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=5.0, description='loc', max=20.0, min=-20.0), FloatSlider(value=2.0, d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    loc=widgets.FloatSlider(min=-20, max=20, step=0.1, value=5),\n",
    "    std=widgets.FloatSlider(min=1, max=50, value=2),\n",
    "    seed=widgets.IntSlider(min=0, max=50, step=1, value=5),\n",
    "    n_samples=widgets.IntSlider(min=5, max=50, step=1, value=10)\n",
    ")\n",
    "def mean_centering_interact_plot(loc=20, std=2.0, seed=42, n_samples=10):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate example data\n",
    "    x = np.random.normal(loc=loc, scale=std, size=(n_samples, 2))  # mean=50, std=10\n",
    "    center = np.mean(x, axis=0)\n",
    "    x_centered = x - center  # mean-centered\n",
    "    # Plot original vs centered\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    plot_transformation(x, x_centered, \"Centered Data\", axs)\n",
    "    fig.legend(loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b0619-7542-4b52-bab3-423b40877a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the numerical features in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147dcb4a-a1f8-4fdf-8d15-3f66151e76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the model with the new feature values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971287ee-f9ae-478d-8f4c-9f0538943801",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Standardization is a data transformation technique that rescales numerical features so they have a mean of zero and a standard deviation of one.\n",
    "\n",
    "$$x' = \\frac{x - \\bar{x}}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c88d836d-5d6e-44ec-9367-39e55d074999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b4c165e1ce4ca9a1a7f881edcd943e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=3.0, description='loc', max=20.0, min=-20.0), FloatSlider(value=2.0, d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    loc=widgets.FloatSlider(min=-20, max=20, step=0.1, value=3),\n",
    "    std=widgets.FloatSlider(min=1, max=50, value=2),\n",
    "    seed=widgets.IntSlider(min=0, max=50, step=1, value=5),\n",
    "    n_samples=widgets.IntSlider(min=10, max=100, step=1, value=20)\n",
    ")\n",
    "def standardization_interact_plot(loc=20, std=2.0, seed=42, n_samples=20):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate example data\n",
    "    x = np.random.normal(loc=loc, scale=std, size=(n_samples, 2))  # mean=50, std=10\n",
    "    center = np.mean(x, axis=0)\n",
    "    x_centered = x - center  # mean-centered\n",
    "    x_standardized = x_centered / np.std(x_centered, axis=0)\n",
    "    \n",
    "    # Plot original vs centered\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    plot_transformation(x, x_standardized, \"Standardized Data\", axs)\n",
    "    fig.legend(loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9854dc-730f-45b5-978d-35f5574f916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the numerical features in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5d52d-fe31-4939-bb51-4c44baefb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the model with the new feature values\n",
    "# fit_and_eval(one_hot_mapping_train_X, trainY, one_hot_mapping_test_X, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892e755-a84d-401f-99c8-5978c5c46789",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Whitening\n",
    "\n",
    "Whitening is a data transformation technique that not only standardizes features to have zero mean and unit variance but also removes correlations between them, producing uncorrelated variables with identity covariance. Thus we are looking for a transformation $W$ that satisfies $W\\Sigma W^T=I$.\n",
    "\n",
    "Whitening can be achieved through several techniques, each with its own approach to decorrelate and scale data. Here, we are going to focus on Cholesky whitening trasformation, but more can be see in the [Additional Materials](#More-Whitening) section. Cholesky whitening will find such a transformation by decomposing $\\Sigma^{-1}$ with Cholesky decomposition.\n",
    "\n",
    "The Cholesky decomposition factorizes a positive definite matrix $\\Sigma^{-1}$ as:\n",
    "$$\\Sigma^{-1}=LL^T,$$\n",
    "where $L$ is a lower-triangular matrix with positive diagonal entries. Then whitening transformation $W$ is defined as:\n",
    "$$W=L.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "818f7af4-3263-434c-a077-953e32916630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80238b10c80d4d2fab41a4265fc2f1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-3.0, description='loc_x', max=20.0, min=-20.0), FloatSlider(value=3.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    loc_x=widgets.FloatSlider(min=-20, max=20, step=0.1, value=-3),\n",
    "    loc_y=widgets.FloatSlider(min=-20, max=20, step=0.1, value=3),\n",
    "    cov=widgets.FloatSlider(min=-0.95, max=0.95, step=0.05, value=0.8),\n",
    "    seed=widgets.IntSlider(min=0, max=50, step=1, value=5),\n",
    "    n_samples=widgets.IntSlider(min=10, max=100, step=1, value=20)\n",
    ")\n",
    "def whitening_interact_plot(loc_x=10, loc_y=10, cov=0.8, seed=42, n_samples=20):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate example data\n",
    "    x = np.random.multivariate_normal([loc_x, loc_y], [[1, cov], [cov, 1]], n_samples)\n",
    "    print(\"Covariance matrix of the original data:\")\n",
    "    print(np.array2string(np.corrcoef(x.T), precision=2, floatmode='fixed'))\n",
    "    # x = np.random.normal(loc=loc, scale=std, size=(n_samples, 2))  # mean=50, std=10\n",
    "    center = np.mean(x, axis=0)\n",
    "    x_centered = x - center  # mean-centered\n",
    "    # Covariance matrix\n",
    "    cov_matrix = np.cov(x_centered, rowvar=False)\n",
    "\n",
    "    # Cholesky:\n",
    "    L = np.linalg.cholesky(np.linalg.pinv(cov_matrix), upper=False)\n",
    "    X_chol_white = x_centered @ L\n",
    "    # print(np.cov(X_chol_white))\n",
    "    print(\"Covariance matrix of Cholesky whitening:\")\n",
    "    print(np.array2string(np.corrcoef(X_chol_white.T), precision=2, floatmode='fixed'))\n",
    "    \n",
    "    # Plot original vs centered\n",
    "    fig, axs = plt.subplots(1, 1, sharex=True, sharey=True)\n",
    "    axs.set_aspect('equal')\n",
    "    fig.set_size_inches(5.5, 4)\n",
    "    axs.set_title('Cholesky whitening')\n",
    "    \n",
    "    plot_transformation(x, X_chol_white, \"Whitened Data\", axs)\n",
    "    # fig.legend(loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))\n",
    "    # remove duplicated legend labels\n",
    "    handles, labels = fig.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    fig.legend(by_label.values(), by_label.keys(), loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb56ac-d6c6-4a32-b9a0-586e5e39fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply whitening transformation to the numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72a9c5-987c-4377-ad34-5dc7a9baa61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the model with the new feature values\n",
    "# fit_and_eval(one_hot_mapping_train_X, trainY, one_hot_mapping_test_X, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b65c56-a9f4-483c-a030-681126056e77",
   "metadata": {},
   "source": [
    "### Min-max scaling\n",
    "\n",
    "Min-max normalization is a technique that rescales the values of a feature to a fixed range, usually `[0, 1]`, by subtracting the minimum value and dividing by the range of the data. This ensures that all features contribute proportionately, preventing variables with larger scales from dominating models.\n",
    "\n",
    "$$x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f9c9f5-5da1-44c7-8816-238d9a47544e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c7996146c64f9ba21649290a0b1940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.0, description='loc', max=20.0, min=-20.0), FloatSlider(value=0.5, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    loc=widgets.FloatSlider(min=-20, max=20, step=0.1, value=-1),\n",
    "    std=widgets.FloatSlider(min=0.1, max=2, step=0.1, value=0.5),\n",
    "    seed=widgets.IntSlider(min=0, max=50, step=1, value=5),\n",
    "    n_samples=widgets.IntSlider(min=10, max=100, step=1, value=20)\n",
    ")\n",
    "def min_max_interact_plot(loc=-1, std=0.5, seed=42, n_samples=20):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate example data\n",
    "    x = np.random.normal(loc=loc, scale=std, size=(n_samples, 2)) \n",
    "    center = np.mean(x, axis=0)    \n",
    "    x_mm = (x - np.min(x, axis=0)) / (np.max(x, axis=0) - np.min(x, axis=0))\n",
    "    \n",
    "    # Plot original vs centered\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    plot_transformation(x, x_mm, \"Min-Max Normalized Data\", axs)\n",
    "    fig.legend(loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5a56e-7d9a-4971-b495-dd945ea724e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply min-max transformation to the numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd369ba2-4e21-4bea-83e9-5d7561ffae12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaulate the model with the new feature values\n",
    "# fit_and_eval(one_hot_mapping_train_X, trainY, one_hot_mapping_test_X, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51f72d-307e-4f8c-a677-26518ef50aa2",
   "metadata": {},
   "source": [
    "### Unit normalization\n",
    "\n",
    "Unit normalization is a data transformation technique that scales each data vector to have a unit norm, typically a length of 1, by dividing the vector by its magnitude. This emphasizes the direction of the data rather than its absolute scale.\n",
    "\n",
    "$$\\mathbf{x}' = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2} = \\frac{\\mathbf{x}}{\\sqrt{\\sum_{i=1}^{d} x_i^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "54d15cf8-43c6-44e3-9f6f-af7aa8d14218",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b681b0fcf23420db8e1b56c648e8c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.0, description='loc', max=20.0, min=-20.0), FloatSlider(value=0.5, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    loc=widgets.FloatSlider(min=-20, max=20, step=0.1, value=-1),\n",
    "    std=widgets.FloatSlider(min=0.1, max=2, step=0.1, value=0.5),\n",
    "    seed=widgets.IntSlider(min=0, max=50, step=1, value=5),\n",
    "    n_samples=widgets.IntSlider(min=10, max=100, step=1, value=20)\n",
    ")\n",
    "def unit_norm_interact_plot(loc=-1, std=0.5, seed=42, n_samples=20, center=False):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate example data\n",
    "    x = np.random.normal(loc=loc, scale=std, size=(n_samples, 2)) \n",
    "    mean = np.mean(x, axis=0)\n",
    "    xc = x\n",
    "    if center:\n",
    "        xc = x - mean\n",
    "    x_unit = xc / np.linalg.norm(xc, axis=1)[:, None]\n",
    "    \n",
    "    # Plot original vs centered\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    plot_transformation(xc, x_unit, \"Unit Normalized Data\", axs)\n",
    "    ticks = axs.get_xticks()\n",
    "    print(axs.get_xticks())\n",
    "    axs.set_xticks(ticks)\n",
    "    axs.set_yticks(ticks)\n",
    "    fig.legend(loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c723157-1d14-4da6-a074-6694920cc081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply unit normalization to the numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14283383-e600-42db-aa11-7fbf1dca411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the model with the new feature values\n",
    "# fit_and_eval(one_hot_mapping_train_X, trainY, one_hot_mapping_test_X, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e53bfe-b489-45de-8877-d5f832c18fdc",
   "metadata": {},
   "source": [
    "# Additional Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5d1f3-8ded-442a-9129-f51b41fbfe40",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Bonferroni’s principle \n",
    "\n",
    "Bonferroni’s principle is a statistical caution that says:\n",
    "> If you keep looking for patterns in data without adjusting your criteria, you’re bound to find \"significant\" results purely by chance.\n",
    "\n",
    "It reminds us that if you search a large enough dataset for correlations, patterns, or anomalies without proper statistical controls, you will almost certainly find patterns that are just random noise. This is especially important when working with high-dimensional data, where the number of possible comparisons is huge.\n",
    "\n",
    "For example, if you test $m=20$ independent hypotheses at a $5%$ significance level ($p < 0.05$), you should expect about 1 false positive even if none of the hypotheses are actually true. Bonferroni’s correction addresses this by lowering the threshold for each test:\n",
    "\n",
    "$$\\alpha'=\\frac{\\alpha}{m}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b01cd2-e513-45d3-a72a-ef6d8be8ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "_max_n_samples=200\n",
    "_max_n_variables=50\n",
    "\n",
    "@interact(\n",
    "    n_samples=widgets.IntSlider(min=5, max=_max_n_samples, step=1, value=100),\n",
    "    n_variables=widgets.IntSlider(min=5, max=_max_n_variables, step=1, value=5)\n",
    ")\n",
    "def bonferroni(n_samples=100, n_variables=20):\n",
    "    np.random.seed(42)\n",
    "    # Generate random data (completely uncorrelated)\n",
    "    data = np.random.randn(n_variables, n_samples)\n",
    "    data = data.T\n",
    "    \n",
    "    alpha = 0.05  # significance level\n",
    "    \n",
    "    false_positives = 0\n",
    "    total_tests = np.sum(np.arange(n_variables))\n",
    "    \n",
    "    # Apply Bonferroni correction\n",
    "    alpha_bonferroni = alpha / total_tests\n",
    "    false_positives_corrected = 0\n",
    "    \n",
    "    # Test all pairs of variables\n",
    "    p_values = []\n",
    "    for i in range(n_variables):\n",
    "        for j in range(i+1, n_variables):\n",
    "            r, p_value = pearsonr(data[:, i], data[:, j])\n",
    "            p_values.append(p_value)\n",
    "            if p_value < alpha:\n",
    "                false_positives += 1\n",
    "            if p_value < alpha_bonferroni:\n",
    "                false_positives_corrected += 1\n",
    "    \n",
    "    p_values = np.array(p_values)\n",
    "    \n",
    "    print(f\"Total tests: {total_tests}\")\n",
    "    print(f\"False positives at α={alpha}: {false_positives}\")\n",
    "    print(f\"False positives after Bonferroni correction (α={alpha_bonferroni}): {false_positives_corrected}\")\n",
    "    print(f\"Adjusted alpha: {alpha_bonferroni:.6f}\")\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    \n",
    "    _n, _, _ = axs[0].hist(p_values)\n",
    "    axs[0].axvline(alpha, 0, np.max(_n)+1, color=\"orange\", label=\"α=0.05\")\n",
    "    axs[0].axvline(alpha_bonferroni, 0, np.max(_n)+1, color=\"red\", label=\"Bonferroni Corrected\")\n",
    "    axs[0].set_title(\"Distribution of P-values\")\n",
    "    \n",
    "    _n, _, _ = axs[1].hist(p_values[p_values<=0.1])\n",
    "    axs[1].axvline(alpha, 0, np.max(_n)+1, color=\"orange\")\n",
    "    axs[1].axvline(alpha_bonferroni, 0, np.max(_n)+1, color=\"red\")\n",
    "    axs[1].set_xlim(-1e-3, 0.1)\n",
    "    axs[1].set_title(\"Zoomed in [0, 0.1]\")\n",
    "    fig.legend(loc='lower center', ncols=2, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080c65c-adba-43cc-ab91-e0119e5ebb21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# following Bonferroni's principle, how does the number of correlated features change?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0bb18-6dd4-429d-8052-2396f964fb39",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## More Whitening\n",
    "\n",
    "| Feature / Aspect           | Cholesky Whitening                                                                 | PCA Whitening                                                                 | ZCA Whitening                                                                 |\n",
    "|----------------------------|----------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|\n",
    "| **Purpose**                | Transform correlated variables into uncorrelated ones using covariance factorization | Decorrelate and normalize data by projecting onto principal components        | Decorrelate and normalize data while keeping it as close as possible to original space |\n",
    "| **Decomposition** | Cholesky: $\\Sigma = LL^T$ | PCA: $\\Sigma = V \\Lambda V^T $ | PCA: $\\Sigma = V \\Lambda V^T $ |\n",
    "| **Transformation** | $x_{\\text{whitened}} = L^{-1}x$               | $x_{\\text{whitened}} = \\Lambda^{-1/2} V^T x $ | $x_{\\text{whitened}} = V \\Lambda^{-1/2} V^T x$                                  |\n",
    "| **Output Correlation**      | Zero (uncorrelated)                                                               | Zero (uncorrelated)                                                           | Zero (uncorrelated)                                                           |\n",
    "| **Output Orientation**      | Depends on Cholesky factor; not aligned with original axes                        | Aligned with principal components; rotated axes                               | Closest to original data orientation; minimal rotation                        |\n",
    "| **Preserves Original Structure?** | No                                                                          | Partially (rotated along principal axes)                                      | Yes, maintains overall data structure                                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "13caaf1d-2622-4c55-9339-f9c528159728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613a2a7429f342799724c02c363cd4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-3.0, description='loc_x', max=20.0, min=-20.0), FloatSlider(value=3.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    loc_x=widgets.FloatSlider(min=-20, max=20, step=0.1, value=-3),\n",
    "    loc_y=widgets.FloatSlider(min=-20, max=20, step=0.1, value=3),\n",
    "    cov=widgets.FloatSlider(min=-0.95, max=0.95, step=0.05, value=0.8),\n",
    "    seed=widgets.IntSlider(min=0, max=50, step=1, value=5),\n",
    "    n_samples=widgets.IntSlider(min=10, max=100, step=1, value=20)\n",
    ")\n",
    "def whitening_interact_plot(loc_x=10, loc_y=10, cov=0.8, seed=42, n_samples=20):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate example data\n",
    "    x = np.random.multivariate_normal([loc_x, loc_y], [[1, cov], [cov, 1]], n_samples)\n",
    "    print(\"Covariance matrix of the original data:\")\n",
    "    print(np.array2string(np.corrcoef(x.T), precision=2, floatmode='fixed'))\n",
    "    # x = np.random.normal(loc=loc, scale=std, size=(n_samples, 2))  # mean=50, std=10\n",
    "    center = np.mean(x, axis=0)\n",
    "    x_centered = x - center  # mean-centered\n",
    "    # Covariance matrix\n",
    "    cov_matrix = np.cov(x_centered, rowvar=False)\n",
    "\n",
    "    # Eigen decomposition\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # PCA Whitening: decorrelates & scales to unit variance\n",
    "    D_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals))\n",
    "    X_pca_white = x_centered @ eigvecs @ D_inv_sqrt\n",
    "\n",
    "    # ZCA whitening: rotates the vectors back to the original orientation\n",
    "    X_zca_white = X_pca_white @ eigvecs.T\n",
    "\n",
    "    # Cholesky:\n",
    "    L = np.linalg.cholesky(np.linalg.pinv(cov_matrix), upper=False)\n",
    "    X_chol_white = x_centered @ L\n",
    "    # print(np.cov(X_chol_white))\n",
    "    print(\"Covariance matrix of Cholesky whitening:\")\n",
    "    print(np.array2string(np.corrcoef(X_chol_white.T), precision=2, floatmode='fixed'))\n",
    "    print(\"Covariance matrix of PCA whitening:\")\n",
    "    print(np.array2string(np.corrcoef(X_pca_white.T), precision=2, floatmode='fixed'))\n",
    "    print(\"Covariance matrix of ZCA whitening:\")\n",
    "    print(np.array2string(np.corrcoef(X_zca_white.T), precision=2, floatmode='fixed'))\n",
    "    \n",
    "    # Plot original vs centered\n",
    "    fig, axs = plt.subplots(1, 3, sharex=True, sharey=True)\n",
    "    axs[0].set_aspect('equal')\n",
    "    axs[1].set_aspect('equal')\n",
    "    axs[2].set_aspect('equal')\n",
    "    fig.set_size_inches(10.5, 4)\n",
    "    axs[1].set_title('PCA whitening')\n",
    "    axs[2].set_title('ZCA whitening')\n",
    "    axs[0].set_title('Cholesky whitening')\n",
    "    \n",
    "    plot_transformation(x, X_chol_white, \"Whitened Data\", axs[0])\n",
    "    plot_transformation(x, X_pca_white, \"Whitened Data\", axs[1])\n",
    "    plot_transformation(x, X_zca_white, \"Whitened Data\", axs[2])\n",
    "    # fig.legend(loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))\n",
    "    # remove duplicated legend labels\n",
    "    handles, labels = fig.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    fig.legend(by_label.values(), by_label.keys(), loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5717153-8158-4ace-93b9-0b45a47330ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply whitening transformation to the numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca5a90-fcfe-4aeb-9a59-943601261694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the model with the new feature values\n",
    "# fit_and_eval(one_hot_mapping_train_X, trainY, one_hot_mapping_test_X, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616fad66-3cd0-45cb-bf7b-a6a3b0eec441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
