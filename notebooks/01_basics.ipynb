{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c48eae2-5997-43f9-af34-9b5bb24fb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518087f-2cb5-4289-8743-dbfc32f7083b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Data Mining\n",
    "\n",
    "Data mining is the process of discovering meaningful patterns, trends, and relationships in large datasets using statistical, machine learning, and database management techniques. It goes beyond simple data analysis by automatically extracting hidden knowledge that can support decision-making, and helps us understand complex phenomena. Common applications include customer behavior analysis, fraud detection, medical diagnosis, or market trend prediction. By turning raw data into actionable insights, data mining serves as a critical tool in today’s data-driven world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2052d-9b87-400d-a1d3-0075be81d982",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Data\n",
    "\n",
    "The most convenient way to think of the datasets that the majority\n",
    "of data mining algorithms operate upon is the tabular view. In this\n",
    "analogy the problem at hand can be treated as (a potentially gigantic)\n",
    "spreadsheet with several rows – corresponding to data objects – and\n",
    "columns, each of which includes observed attributes with respect the\n",
    "different aspects of these data objects.\n",
    "\n",
    "Another important aspect of the datasets we work with is the\n",
    "measurement scale of the individual columns in the data matrix\n",
    "(each corresponding to a random variable). A concise summary of\n",
    "the different measurement scales and some of the most prototypical\n",
    "statistics which can be calculated for them:\n",
    "\n",
    "| Type of attribute | Description | Examples | Statistics |\n",
    "|-------------------|-------------|----------|------------|\n",
    "| **Categorical**   |             |          |            |\n",
    "| Nominal           | Variables can be checked for equality only; | names of cities, hair color | mode, entropy, correlation, χ²-test |\n",
    "| Ordinal           |  `>` relation can be interpreted among variables; | grades {fail, pass, excellent} | median, percentiles |\n",
    "| **Numerical**     |             |          |            |\n",
    "| Interval          | The difference of two variables can be formed and interpreted | shoe sizes, dates, °C | mean, deviation, significance (e.g., F-, t- tests) |\n",
    "| Ratio             | Ratios can be formed from values of the variables of this kind | age, length, temperature in Kelvin | percent, geometric/harmonic mean, variation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeadae99-d7fc-47d7-b3fd-6aa187b00c08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's load the [Bike Rental Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/bike-sharing-dataset). A slightly modified version that we are going to use can be found at `/data/rental.csv`.\n",
    "\n",
    "Dataset features:\n",
    "- `season`: Season\n",
    "- `yr`: Year\n",
    "- `mnth`: Month\n",
    "- `holiday`: Indicator whether the day was a holiday or not.\n",
    "- `weekday`: Day of the week.\n",
    "- `workingday`: Indicator whether the day was a working day or weekend.\n",
    "- `weathersit`: The weather situation on that day. One of:\n",
    "  - 1: clear, few clouds, partly cloudy, cloudy\n",
    "  - 2: mist + clouds, mist + broken clouds, mist + few clouds, mist\n",
    "  - 3: light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds\n",
    "  - 4: heavy rain + ice pallets + thunderstorm + mist, snow + mist\n",
    "- `temp`: Temperature in degrees Celsius.\n",
    "- `atemp`: Felt temperature in Celsius.\n",
    "- `hum`: Relative humidity in percent (0 to 100).\n",
    "- `windspeed`: Wind speed in km per hour.\n",
    "- `cnt`: Count of bicycles including both casual and registered users. The count is used as the target in the regression task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1a45931b-98b8-4c3f-b3f2-726aa5feefae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>january</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.175849</td>\n",
       "      <td>39.999250</td>\n",
       "      <td>80.5833</td>\n",
       "      <td>10.749882</td>\n",
       "      <td>985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>january</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.083466</td>\n",
       "      <td>39.346774</td>\n",
       "      <td>69.6087</td>\n",
       "      <td>16.652113</td>\n",
       "      <td>801.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>january</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.229108</td>\n",
       "      <td>28.500730</td>\n",
       "      <td>43.7273</td>\n",
       "      <td>16.636703</td>\n",
       "      <td>1349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>january</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>30.000052</td>\n",
       "      <td>59.0435</td>\n",
       "      <td>10.739832</td>\n",
       "      <td>1562.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>january</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.666979</td>\n",
       "      <td>31.131820</td>\n",
       "      <td>43.6957</td>\n",
       "      <td>12.522300</td>\n",
       "      <td>1600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>december</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.945849</td>\n",
       "      <td>30.958372</td>\n",
       "      <td>65.2917</td>\n",
       "      <td>23.458911</td>\n",
       "      <td>2114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>december</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.906651</td>\n",
       "      <td>32.833036</td>\n",
       "      <td>59.0000</td>\n",
       "      <td>10.416557</td>\n",
       "      <td>3095.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>december</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.906651</td>\n",
       "      <td>31.998400</td>\n",
       "      <td>75.2917</td>\n",
       "      <td>8.333661</td>\n",
       "      <td>1341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>december</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.024151</td>\n",
       "      <td>31.292200</td>\n",
       "      <td>48.3333</td>\n",
       "      <td>23.500518</td>\n",
       "      <td>1796.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>december</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.144151</td>\n",
       "      <td>30.750142</td>\n",
       "      <td>57.7500</td>\n",
       "      <td>10.374682</td>\n",
       "      <td>2729.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>731 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     season    yr      mnth  holiday  weekday  workingday  weathersit  \\\n",
       "0    spring  2011   january      0.0      6.0         0.0         2.0   \n",
       "1    spring  2011   january      0.0      0.0         0.0         2.0   \n",
       "2    spring  2011   january      0.0      1.0         1.0         1.0   \n",
       "3    spring  2011   january      0.0      2.0         1.0         1.0   \n",
       "4    spring  2011   january      0.0      3.0         1.0         1.0   \n",
       "..      ...   ...       ...      ...      ...         ...         ...   \n",
       "726  spring  2012  december      0.0      4.0         1.0         2.0   \n",
       "727  spring  2012  december      0.0      5.0         1.0         2.0   \n",
       "728  spring  2012  december      0.0      6.0         0.0         2.0   \n",
       "729  spring  2012  december      0.0      0.0         0.0         1.0   \n",
       "730  spring  2012  december      0.0      1.0         1.0         2.0   \n",
       "\n",
       "          temp      atemp      hum  windspeed     cnt  \n",
       "0    24.175849  39.999250  80.5833  10.749882   985.0  \n",
       "1    25.083466  39.346774  69.6087  16.652113   801.0  \n",
       "2    17.229108  28.500730  43.7273  16.636703  1349.0  \n",
       "3    17.400000  30.000052  59.0435  10.739832  1562.0  \n",
       "4    18.666979  31.131820  43.6957  12.522300  1600.0  \n",
       "..         ...        ...      ...        ...     ...  \n",
       "726  19.945849  30.958372  65.2917  23.458911  2114.0  \n",
       "727  19.906651  32.833036  59.0000  10.416557  3095.0  \n",
       "728  19.906651  31.998400  75.2917   8.333661  1341.0  \n",
       "729  20.024151  31.292200  48.3333  23.500518  1796.0  \n",
       "730  18.144151  30.750142  57.7500  10.374682  2729.0  \n",
       "\n",
       "[731 rows x 12 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "df = pd.read_csv(\"https://github.com/ficstamas/data-mining/raw/b76d5b7913c446878fa47de8861c83e26780828f/data/rental.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ad4fd7c7-16ea-4332-878c-128af3ca8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the train-test splits and separate the target variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_X, train_y = train[train.columns.difference([\"cnt\"])], train[[\"cnt\"]]\n",
    "test_X, test_y = test[test.columns.difference([\"cnt\"])], test[[\"cnt\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7104b8-2a68-4d9e-af8b-5086a89b12dc",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e772a4-8830-447a-895f-8a90d38d19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a histogram visualize the distribution of numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921636a-c6ac-4115-98ee-5688911f4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the joint behaviour of two variables using scatter plot\n",
    "# extra: include the histogram of the the point next to the x and y axes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd63698b-d2a4-46ac-b39e-3f1047d987a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the distribution of windspeed for each season (using box or violin plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c0710-7c4f-4123-98fd-8b24f6a75116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualize any interesing asapect of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130eafb-cb8a-4788-901f-0af9027a3ccd",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2af01340-40d6-4295-946b-65178e3ef64a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model fitting and evaluation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def fit_and_eval(trainX, trainY, testX, testY):\n",
    "    model = LinearRegression()\n",
    "    model.fit(trainX, trainY)\n",
    "    \n",
    "    predictions = model.predict(testX)\n",
    "    return r2_score(testY, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7c44d-a234-4a95-8b67-f66baa8baf89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "Data preprocessing is a crucial step in any data analysis or machine learning workflow because raw data is often incomplete, inconsistent, noisy, or in a format unsuitable for modeling. Preprocessing transforms this raw data into a clean, well-structured form, ensuring that the analysis or model can extract meaningful patterns rather than being misled by errors or irrelevant information. This means that we perform some transformation over the data matrix, either in a column or a row-oriented manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7342229-4f64-4acf-8d41-df76fb5b29b4",
   "metadata": {},
   "source": [
    "## Categorical data\n",
    "\n",
    "We should transform each nominal features into numerical ones, since not every model can handle it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7a6ed-1db7-409e-8474-3abd18ecbac4",
   "metadata": {},
   "source": [
    "### Numeric mapping\n",
    "\n",
    "It means that we map each categorical value to a numerical value, for example instead representing `seasons` as strings, we remap each value in the following manner `{'spring': 0, 'summer': 1, 'fall': 2, 'winter': 3}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce7b70-c0c8-42e9-9ee6-94cc932cfa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets transform each categorical feature to their numerical representation\n",
    "# \n",
    "def _transform_categorical_to_numerical(row):\n",
    "    \"\"\"\n",
    "    row: A row of the dataset. You can access a column by `row.column_name` or row[\"column_name\"]\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    return row\n",
    "\n",
    "\n",
    "numeric_mapping_train_X = train_X.apply(_transform_categorical_to_numerical, axis=1)\n",
    "numeric_mapping_test_X = test_X.apply(_transform_categorical_to_numerical, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7556a-f506-4006-b506-06e9afc0f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "fit_and_eval(numeric_mapping_train_X, trainY, numeric_mapping_test_X, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32277d64-1897-40ab-af61-5b39a198a68d",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6b1b0-18f4-415a-afe2-20b4c198261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets transform each categorical feature to their one-hot encoded representation\n",
    "# \n",
    "\n",
    "\n",
    "one_hot_mapping_train_X = None\n",
    "one_hot_mapping_test_X = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dba0cb-2da8-464c-afc4-dff6552ffe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_eval(one_hot_mapping_train_X, trainY, one_hot_mapping_test_X, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6220961c-0ad0-4a01-9b1e-b82006def6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to remove dummy variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0834096-314a-4847-8e97-a8475c830cbe",
   "metadata": {},
   "source": [
    "## Numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1c2fef35-e74a-4e13-b227-d13f920bddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for plotting\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# Khachiyan Algorithm for MVEE\n",
    "def mvee(points, tol=1e-3):\n",
    "    N, d = points.shape\n",
    "    Q = np.column_stack((points, np.ones(N)))  # Add ones for affine transformation\n",
    "    u = np.ones(N) / N  # initial uniform weights\n",
    "\n",
    "    while True:\n",
    "        # Matrix X = Q^T * diag(u) * Q\n",
    "        X = Q.T @ np.diag(u) @ Q\n",
    "        M = np.diag(Q @ np.linalg.inv(X) @ Q.T)  # Mahalanobis distances\n",
    "        j = np.argmax(M)\n",
    "        max_M = M[j]\n",
    "        step_size = (max_M - d - 1) / ((d + 1) * (max_M - 1))\n",
    "        new_u = (1 - step_size) * u\n",
    "        new_u[j] += step_size\n",
    "        if np.linalg.norm(new_u - u) < tol:\n",
    "            break\n",
    "        u = new_u\n",
    "\n",
    "    # Center\n",
    "    center = u @ points\n",
    "    # Covariance matrix of the ellipse\n",
    "    A = np.linalg.inv(points.T @ np.diag(u) @ points - np.outer(center, center)) / d\n",
    "    return center, A\n",
    "\n",
    "\n",
    "# Function to plot ellipse from center and matrix A\n",
    "def plot_enclosing_ellipse(x, ax, eps=0, **kwargs):\n",
    "    hull = ConvexHull(x)\n",
    "    hull_points = x[hull.vertices]\n",
    "    center, A = mvee(hull_points)\n",
    "    U, s, Vt = np.linalg.svd(A)\n",
    "    angle = np.degrees(np.arctan2(U[1,0], U[0,0]))\n",
    "    width, height = 2 / np.sqrt(s) + eps  # radii\n",
    "    ell = Ellipse(xy=center, width=width, height=height, angle=angle, **kwargs)\n",
    "    ax.add_patch(ell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b71a86-fbb3-44ed-8fb9-2f2f1d641050",
   "metadata": {},
   "source": [
    "### Mean centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d203d0d-5328-4122-b2f3-930b3d3e8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of a random vector without using any builtin or numpy functions\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(50)\n",
    "\n",
    "# code here X_std should contain the final result\n",
    "X_std = None\n",
    "\n",
    "assert np.allclose(np.std(X), X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "79452d73-895e-40dc-9ffa-49ac8ca08561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3112cbed2b1c48ad8b18e3c6f014bcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=5.0, description='loc', max=20.0, min=-20.0), FloatSlider(value=2.0, d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    loc=widgets.FloatSlider(min=-20, max=20, step=0.1, value=5),\n",
    "    std=widgets.FloatSlider(min=1, max=50, value=2),\n",
    "    seed=widgets.IntSlider(min=0, max=50, step=1, value=5),\n",
    "    n_samples=widgets.IntSlider(min=5, max=50, step=1, value=10)\n",
    ")\n",
    "def mean_centering_interact_plot(loc=20, std=2.0, seed=42, n_samples=10):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate example data\n",
    "    x = np.random.normal(loc=loc, scale=std, size=(n_samples, 2))  # mean=50, std=10\n",
    "    center = np.mean(x, axis=0)\n",
    "    x_centered = x - center  # mean-centered\n",
    "    \n",
    "    # Plot original vs centered\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    \n",
    "    axs.scatter(x[:, 0], x[:, 1], color=\"red\", label=\"Original Data\")\n",
    "    axs.scatter(center[0], center[1], marker=\"x\", color=\"blue\", label=\"Mean\")\n",
    "    \n",
    "    plot_enclosing_ellipse(x, axs, fill=False, color='red', linewidth=1)\n",
    "    \n",
    "\n",
    "    _m_centered = np.mean(x_centered, axis=0)\n",
    "    axs.scatter(x_centered[:, 0], x_centered[:, 1], color=\"yellow\", label=\"Centered Data\")\n",
    "    axs.scatter(_m_centered[0], _m_centered[1], marker=\"x\", color=\"blue\")\n",
    "    plot_enclosing_ellipse(x_centered, axs, fill=False, color='yellow', linewidth=1)\n",
    "    \n",
    "    axs.annotate(\"\", xytext=(center[0], center[1]), xy=(_m_centered[0], _m_centered[1]), arrowprops=dict(arrowstyle=\"->\", color=\"red\"))\n",
    "    fig.legend(loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b0619-7542-4b52-bab3-423b40877a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the numerical features in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147dcb4a-a1f8-4fdf-8d15-3f66151e76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the model with the new feature values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971287ee-f9ae-478d-8f4c-9f0538943801",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b2535f-5fb1-4843-9ca2-036aa7e8b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standarde deviation of a random vector without using any builtin or numpy functions\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(50)\n",
    "\n",
    "# code here X_std should contain the final result\n",
    "X_std = None\n",
    "\n",
    "assert np.allclose(np.std(X), X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c88d836d-5d6e-44ec-9367-39e55d074999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39445c75cba4242b7a00ca170b8f8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=3.0, description='loc', max=20.0, min=-20.0), FloatSlider(value=2.0, d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    loc=widgets.FloatSlider(min=-20, max=20, step=0.1, value=3),\n",
    "    std=widgets.FloatSlider(min=1, max=50, value=2),\n",
    "    seed=widgets.IntSlider(min=0, max=50, step=1, value=5),\n",
    "    n_samples=widgets.IntSlider(min=10, max=100, step=1, value=20)\n",
    ")\n",
    "def standardization_interact_plot(loc=20, std=2.0, seed=42, n_samples=20):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate example data\n",
    "    x = np.random.normal(loc=loc, scale=std, size=(n_samples, 2))  # mean=50, std=10\n",
    "    center = np.mean(x, axis=0)\n",
    "    x_centered = x - center  # mean-centered\n",
    "    x_standardized = x_centered / np.std(x_centered, axis=0)\n",
    "    \n",
    "    # Plot original vs centered\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    \n",
    "    axs.scatter(x[:, 0], x[:, 1], color=\"red\", label=\"Original Data\")\n",
    "    axs.scatter(center[0], center[1], marker=\"x\", color=\"blue\", label=\"Mean\")\n",
    "    \n",
    "    plot_enclosing_ellipse(x, axs, fill=False, color='red', linewidth=1)\n",
    "    \n",
    "\n",
    "    _m_centered = np.mean(x_standardized, axis=0)\n",
    "    axs.scatter(x_standardized[:, 0], x_standardized[:, 1], color=\"yellow\", label=\"Standardized Data\")\n",
    "    axs.scatter(_m_centered[0], _m_centered[1], marker=\"x\", color=\"blue\")\n",
    "    plot_enclosing_ellipse(x_standardized, axs, fill=False, color='yellow', linewidth=1)\n",
    "    \n",
    "    axs.annotate(\"\", xytext=(center[0], center[1]), xy=(_m_centered[0], _m_centered[1]), arrowprops=dict(arrowstyle=\"->\", color=\"red\"))\n",
    "    fig.legend(loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9854dc-730f-45b5-978d-35f5574f916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the numerical features in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5d52d-fe31-4939-bb51-4c44baefb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the model with the new feature values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892e755-a84d-401f-99c8-5978c5c46789",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c7bfeaae-e0eb-4213-834a-56ac024859c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 30)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the covariance matrix without using any builtin functions\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 30)\n",
    "\n",
    "# code here X_cov should contain the final result\n",
    "X_cov = None\n",
    "\n",
    "assert np.allclose(np.cov(X), X_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "13caaf1d-2622-4c55-9339-f9c528159728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e5b34a3b58496e9467a777018d1416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-3.0, description='loc', max=20.0, min=-20.0), FloatSlider(value=0.8, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    loc=widgets.FloatSlider(min=-20, max=20, step=0.1, value=-3),\n",
    "    cov=widgets.FloatSlider(min=0, max=0.9, step=0.05, value=0.8),\n",
    "    seed=widgets.IntSlider(min=0, max=50, step=1, value=5),\n",
    "    n_samples=widgets.IntSlider(min=10, max=100, step=1, value=20)\n",
    ")\n",
    "def whitening_interact_plot(loc=20, cov=0.8, seed=42, n_samples=20):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate example data\n",
    "    x = np.random.multivariate_normal([loc, loc], [[1, cov], [cov, 1]], n_samples)\n",
    "    # x = np.random.normal(loc=loc, scale=std, size=(n_samples, 2))  # mean=50, std=10\n",
    "    center = np.mean(x, axis=0)\n",
    "    x_centered = x - center  # mean-centered\n",
    "    # Covariance matrix\n",
    "    cov_matrix = np.cov(x_centered, rowvar=False)\n",
    "\n",
    "    # Eigen decomposition\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # PCA Whitening: decorrelates & scales to unit variance\n",
    "    D_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals))\n",
    "    X_pca_white = x_centered @ eigvecs @ D_inv_sqrt\n",
    "\n",
    "    # ZCA whitening: rotates the vectors back to the original orientation\n",
    "    X_zca_white = X_pca_white @ eigvecs.T\n",
    "\n",
    "    # Cholesky:\n",
    "    L = np.linalg.cholesky(np.linalg.pinv(cov_matrix), upper=False)\n",
    "    X_chol_white = x_centered @ L\n",
    "    # print(np.cov(X_chol_white))\n",
    "    \n",
    "    # Plot original vs centered\n",
    "    fig, axs = plt.subplots(1, 3, sharex=True, sharey=True)\n",
    "    axs[0].set_aspect('equal')\n",
    "    axs[1].set_aspect('equal')\n",
    "    axs[2].set_aspect('equal')\n",
    "    fig.set_size_inches(10.5, 4)\n",
    "    axs[1].set_title('PCA whitening')\n",
    "    axs[2].set_title('ZCA whitening')\n",
    "    axs[0].set_title('Cholesky whitening')\n",
    "\n",
    "    # PCA plot\n",
    "    axs[1].scatter(x[:, 0], x[:, 1], color=\"red\", label=\"Original Data\")\n",
    "    axs[1].scatter(center[0], center[1], marker=\"x\", color=\"blue\", label=\"Mean\")\n",
    "    \n",
    "    plot_enclosing_ellipse(x, axs[1], fill=False, color='red', linewidth=1)\n",
    "    \n",
    "\n",
    "    _m_centered = np.mean(X_pca_white, axis=0)\n",
    "    axs[1].scatter(X_pca_white[:, 0], X_pca_white[:, 1], color=\"yellow\", label=\"Whitened Data\")\n",
    "    axs[1].scatter(_m_centered[0], _m_centered[1], marker=\"x\", color=\"blue\")\n",
    "    plot_enclosing_ellipse(X_pca_white, axs[1], fill=False, color='yellow', linewidth=1)\n",
    "    \n",
    "    axs[1].annotate(\"\", xytext=(center[0], center[1]), xy=(_m_centered[0], _m_centered[1]), arrowprops=dict(arrowstyle=\"->\", color=\"red\"))\n",
    "    \n",
    "\n",
    "    # ZCA plot\n",
    "\n",
    "    axs[2].scatter(x[:, 0], x[:, 1], color=\"red\")\n",
    "    axs[2].scatter(center[0], center[1], marker=\"x\", color=\"blue\")\n",
    "    \n",
    "    plot_enclosing_ellipse(x, axs[2], fill=False, color='red', linewidth=1)\n",
    "    \n",
    "\n",
    "    _m_centered = np.mean(X_zca_white, axis=0)\n",
    "    axs[2].scatter(X_zca_white[:, 0], X_zca_white[:, 1], color=\"yellow\")\n",
    "    axs[2].scatter(_m_centered[0], _m_centered[1], marker=\"x\", color=\"blue\")\n",
    "    plot_enclosing_ellipse(X_zca_white, axs[2], fill=False, color='yellow', linewidth=1)\n",
    "    \n",
    "    axs[2].annotate(\"\", xytext=(center[0], center[1]), xy=(_m_centered[0], _m_centered[1]), arrowprops=dict(arrowstyle=\"->\", color=\"red\"))\n",
    "    fig.legend(loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "    # Cholesky plot\n",
    "\n",
    "    axs[0].scatter(x[:, 0], x[:, 1], color=\"red\")\n",
    "    axs[0].scatter(center[0], center[1], marker=\"x\", color=\"blue\")\n",
    "    \n",
    "    plot_enclosing_ellipse(x, axs[0], fill=False, color='red', linewidth=1)\n",
    "\n",
    "    _m_centered = np.mean(X_chol_white, axis=0)\n",
    "    axs[0].scatter(X_chol_white[:, 0], X_chol_white[:, 1], color=\"yellow\")\n",
    "    axs[0].scatter(_m_centered[0], _m_centered[1], marker=\"x\", color=\"blue\")\n",
    "    plot_enclosing_ellipse(X_chol_white, axs[0], fill=False, color='yellow', linewidth=1)\n",
    "    \n",
    "    axs[0].annotate(\"\", xytext=(center[0], center[1]), xy=(_m_centered[0], _m_centered[1]), arrowprops=dict(arrowstyle=\"->\", color=\"red\"))\n",
    "    fig.legend(loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5717153-8158-4ace-93b9-0b45a47330ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply whitening transformation to the numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca5a90-fcfe-4aeb-9a59-943601261694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the model with the new feature values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b65c56-a9f4-483c-a030-681126056e77",
   "metadata": {},
   "source": [
    "### Min-max scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a1f9c9f5-5da1-44c7-8816-238d9a47544e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b10993ceba415aa03ec59e62f5cd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.0, description='loc', max=20.0, min=-20.0), FloatSlider(value=0.5, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    loc=widgets.FloatSlider(min=-20, max=20, step=0.1, value=-1),\n",
    "    std=widgets.FloatSlider(min=0.1, max=2, step=0.1, value=0.5),\n",
    "    seed=widgets.IntSlider(min=0, max=50, step=1, value=5),\n",
    "    n_samples=widgets.IntSlider(min=10, max=100, step=1, value=20)\n",
    ")\n",
    "def min_max_interact_plot(loc=-1, std=0.5, seed=42, n_samples=20):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate example data\n",
    "    x = np.random.normal(loc=loc, scale=std, size=(n_samples, 2)) \n",
    "    center = np.mean(x, axis=0)    \n",
    "    x_mm = (x - np.min(x, axis=0)) / (np.max(x, axis=0) - np.min(x, axis=0))\n",
    "    \n",
    "    # Plot original vs centered\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    \n",
    "    axs.scatter(x[:, 0], x[:, 1], color=\"red\", label=\"Original Data\")\n",
    "    axs.scatter(center[0], center[1], marker=\"x\", color=\"blue\", label=\"Mean\")\n",
    "    \n",
    "    plot_enclosing_ellipse(x, axs, fill=False, color='red', linewidth=1)\n",
    "    \n",
    "\n",
    "    _m_centered = np.mean(x_mm, axis=0)\n",
    "    axs.scatter(x_mm[:, 0], x_mm[:, 1], color=\"yellow\", label=\"Min-Max Normalized Data\")\n",
    "    axs.scatter(_m_centered[0], _m_centered[1], marker=\"x\", color=\"blue\")\n",
    "    plot_enclosing_ellipse(x_mm, axs, fill=False, color='yellow', linewidth=1)\n",
    "    \n",
    "    axs.annotate(\"\", xytext=(center[0], center[1]), xy=(_m_centered[0], _m_centered[1]), arrowprops=dict(arrowstyle=\"->\", color=\"red\"))\n",
    "    fig.legend(loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5a56e-7d9a-4971-b495-dd945ea724e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply min-max transformation to the numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd369ba2-4e21-4bea-83e9-5d7561ffae12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaulate the model with the new feature values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51f72d-307e-4f8c-a677-26518ef50aa2",
   "metadata": {},
   "source": [
    "### Unit normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "54d15cf8-43c6-44e3-9f6f-af7aa8d14218",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0568e66bf94ce8b48f892930b81ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.0, description='loc', max=20.0, min=-20.0), FloatSlider(value=0.5, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    loc=widgets.FloatSlider(min=-20, max=20, step=0.1, value=-1),\n",
    "    std=widgets.FloatSlider(min=0.1, max=2, step=0.1, value=0.5),\n",
    "    seed=widgets.IntSlider(min=0, max=50, step=1, value=5),\n",
    "    n_samples=widgets.IntSlider(min=10, max=100, step=1, value=20)\n",
    ")\n",
    "def unit_norm_interact_plot(loc=-1, std=0.5, seed=42, n_samples=20):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate example data\n",
    "    x = np.random.normal(loc=loc, scale=std, size=(n_samples, 2)) \n",
    "    center = np.mean(x, axis=0)    \n",
    "    x_unit = x / np.linalg.norm(x, axis=1)[:, None]\n",
    "    \n",
    "    # Plot original vs centered\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    \n",
    "    axs.scatter(x[:, 0], x[:, 1], color=\"red\", label=\"Original Data\")\n",
    "    axs.scatter(center[0], center[1], marker=\"x\", color=\"blue\", label=\"Mean\")\n",
    "    \n",
    "    plot_enclosing_ellipse(x, axs, fill=False, color='red', linewidth=1)\n",
    "    \n",
    "\n",
    "    _m_centered = np.mean(x_unit, axis=0)\n",
    "    axs.scatter(x_unit[:, 0], x_unit[:, 1], color=\"yellow\", label=\"Min-Max Normalized Data\")\n",
    "    axs.scatter(_m_centered[0], _m_centered[1], marker=\"x\", color=\"blue\")\n",
    "    plot_enclosing_ellipse(x_unit, axs, fill=False, color='yellow', linewidth=1)\n",
    "    \n",
    "    axs.annotate(\"\", xytext=(center[0], center[1]), xy=(_m_centered[0], _m_centered[1]), arrowprops=dict(arrowstyle=\"->\", color=\"red\"))\n",
    "    fig.legend(loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c723157-1d14-4da6-a074-6694920cc081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply unit normalization to the numeric features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14283383-e600-42db-aa11-7fbf1dca411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the model with the new feature values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb46f07-93c3-4319-8bc4-72c322015510",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Basic Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266dab23-5390-49d0-89e8-4fdef64aa3e4",
   "metadata": {},
   "source": [
    "## Correlation and Causuality\n",
    "\n",
    "**Correlation refers to a statistical relationship between two variables** - when changes in one variable are associated with changes in another. For example, ice cream sales and beach attendance often rise together, showing a positive correlation. **Causality, on the other hand, means that one event directly influences or produces another.** If A causes B, then changing A will lead to a predictable change in B. While correlation can hint at possible causal links, it does not prove them.\n",
    "\n",
    "The key difference is that correlation simply describes a relationship, while causality explains the underlying mechanism of that relationship. Many correlated events share a common cause or are influenced by other variables (confounders). For instance, both ice cream sales and drowning incidents increase in summer, but the cause is warmer weather - not ice cream itself.\n",
    "\n",
    "**A common misconception is assuming that \"correlation implies causation\".** This error, sometimes called the [post hoc fallacy](https://en.wikipedia.org/wiki/Post_hoc_ergo_propter_hoc), can lead to flawed conclusions in research, business, and policy-making. Proper causal inference requires careful experimental design, statistical controls, or methods like randomized controlled trials, not just observational data. In short: correlation can point you toward possible causes, but causality must be proven through deeper investigation.\n",
    "\n",
    "[It is not that hard to find missleading examples!](https://www.tylervigen.com/spurious-correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "84f8a7bf-3435-4f76-865e-d3ca298b2b0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform correlation analysis on the features\n",
    "# from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5d1f3-8ded-442a-9129-f51b41fbfe40",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Bonferroni’s principle \n",
    "\n",
    "Bonferroni’s principle is a statistical caution that says:\n",
    "> If you keep looking for patterns in data without adjusting your criteria, you’re bound to find \"significant\" results purely by chance.\n",
    "\n",
    "It reminds us that if you search a large enough dataset for correlations, patterns, or anomalies without proper statistical controls, you will almost certainly find patterns that are just random noise. This is especially important when working with high-dimensional data, where the number of possible comparisons is huge.\n",
    "\n",
    "For example, if you test $m=20$ independent hypotheses at a $5%$ significance level ($p < 0.05$), you should expect about 1 false positive even if none of the hypotheses are actually true. Bonferroni’s correction addresses this by lowering the threshold for each test:\n",
    "\n",
    "$$\\alpha'=\\frac{\\alpha}{m}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10b75817-d118-4109-bdd2-311e5ac97462",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6077d6835bcd4462ad82128808f43241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='n_samples', max=200, min=5), IntSlider(value=5, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "_max_n_samples=200\n",
    "_max_n_variables=50\n",
    "\n",
    "@interact(\n",
    "    n_samples=widgets.IntSlider(min=5, max=_max_n_samples, step=1, value=100),\n",
    "    n_variables=widgets.IntSlider(min=5, max=_max_n_variables, step=1, value=5)\n",
    ")\n",
    "def bonferroni(n_samples=100, n_variables=20):\n",
    "    np.random.seed(42)\n",
    "    # Generate random data (completely uncorrelated)\n",
    "    data = np.random.randn(n_variables, n_samples)\n",
    "    data = data.T\n",
    "    \n",
    "    alpha = 0.05  # significance level\n",
    "    \n",
    "    false_positives = 0\n",
    "    total_tests = np.sum(np.arange(n_variables))\n",
    "    \n",
    "    # Apply Bonferroni correction\n",
    "    alpha_bonferroni = alpha / total_tests\n",
    "    false_positives_corrected = 0\n",
    "    \n",
    "    # Test all pairs of variables\n",
    "    p_values = []\n",
    "    for i in range(n_variables):\n",
    "        for j in range(i+1, n_variables):\n",
    "            r, p_value = pearsonr(data[:, i], data[:, j])\n",
    "            p_values.append(p_value)\n",
    "            if p_value < alpha:\n",
    "                false_positives += 1\n",
    "            if p_value < alpha_bonferroni:\n",
    "                false_positives_corrected += 1\n",
    "    \n",
    "    p_values = np.array(p_values)\n",
    "    \n",
    "    print(f\"Total tests: {total_tests}\")\n",
    "    print(f\"False positives at α={alpha}: {false_positives}\")\n",
    "    print(f\"False positives after Bonferroni correction (α={alpha_bonferroni}): {false_positives_corrected}\")\n",
    "    print(f\"Adjusted alpha: {alpha_bonferroni:.6f}\")\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    \n",
    "    _n, _, _ = axs[0].hist(p_values)\n",
    "    axs[0].axvline(alpha, 0, np.max(_n)+1, color=\"orange\", label=\"α=0.05\")\n",
    "    axs[0].axvline(alpha_bonferroni, 0, np.max(_n)+1, color=\"red\", label=\"Bonferroni Corrected\")\n",
    "    axs[0].set_title(\"Distribution of P-values\")\n",
    "    \n",
    "    _n, _, _ = axs[1].hist(p_values[p_values<=0.1])\n",
    "    axs[1].axvline(alpha, 0, np.max(_n)+1, color=\"orange\")\n",
    "    axs[1].axvline(alpha_bonferroni, 0, np.max(_n)+1, color=\"red\")\n",
    "    axs[1].set_xlim(-1e-3, 0.1)\n",
    "    axs[1].set_title(\"Zoomed in [0, 0.1]\")\n",
    "    fig.legend(loc='lower center', ncols=2, bbox_to_anchor=(0.5, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080c65c-adba-43cc-ab91-e0119e5ebb21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# following Bonferroni's principle, how does the number of correlated features change?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc4240-54e9-43f2-9dbb-c2fec70d300e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162376dd-c74b-485b-9d0b-1d3d68588512",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "345e5735-387f-480d-88e9-590f0a2cf2d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11966e87-3d8c-49a0-97a4-84b31cf30a4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
