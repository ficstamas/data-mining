{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e608d7e",
   "metadata": {},
   "source": [
    "# Frequent Itemset Mining\n",
    "\n",
    "In many domains -- from retail to web analytics, from bioinformatics to cybersecurity -- we are surrounded by discrete event data: logs of things that co-occur, purchases that happen together, or features that frequently appear in patterns. The **goal of frequent pattern mining** (or frequent data mining) **is to uncover regularities, co-occurrences, and associations hidden in these large transactional datasets**.\n",
    "\n",
    "\n",
    "**Market Basket Analysis**\n",
    "- Discover that customers who buy milk and bread often also buy butter.\n",
    "- This insight drives store layout, cross-selling, and recommendation systems.\n",
    "\n",
    "**Web & Clickstream Mining**\n",
    "- Identify frequent sequences of pages or actions, e.g., home → search → checkout.\n",
    "- Helps optimize navigation design or ad placement.\n",
    "\n",
    "**Healthcare & Bioinformatics**\n",
    "- Find frequent combinations of symptoms or genes associated with conditions.\n",
    "- Enables diagnostic rule discovery and biomarker identification.\n",
    "\n",
    "**Network Security**\n",
    "- Detect frequent combinations of log events or packet signatures indicative of an attack pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b306667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations, chain\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def powerset(iterable):\n",
    "    '''Return all non-empty proper subsets of an iterable as frozensets.'''\n",
    "    s = list(iterable)\n",
    "    for r in range(1, len(s)):\n",
    "        for comb in combinations(s, r):\n",
    "            yield frozenset(comb)\n",
    "\n",
    "def format_itemset(iset):\n",
    "    return \"{\" + \", \".join(sorted(map(str, iset))) + \"}\"\n",
    "\n",
    "def print_itemsets(freq_dict, num_transactions, max_items=20):\n",
    "    '''Pretty-print up to `max_items` itemsets with support as fraction and count.'''\n",
    "    items = list(freq_dict.items())\n",
    "    items.sort(key=lambda kv: (-kv[1], sorted(list(kv[0]))))\n",
    "    for i, (iset, cnt) in enumerate(items[:max_items], start=1):\n",
    "        sup = cnt / max(1, num_transactions)\n",
    "        print(f\"{i:>2}. {format_itemset(iset):<40} support={sup:.3f} (count={cnt})\")\n",
    "\n",
    "def without(iterable, item):\n",
    "    '''Return iterable minus a single item.'''\n",
    "    return [x for x in iterable if x != item]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b13cc9",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "### Items & Transactions\n",
    "- **Item**: an atomic symbol (e.g., `\"milk\"`, `\"bread\"`, `\"diapers\"`).\n",
    "- **Itemset** ($\\mathcal{I}$): a set of items, e.g., `{milk, bread}`.\n",
    "- **Transaction** ($\\mathcal{T}$): a set (or list) of items e.g., items purchased/observed together.\n",
    "- **Transaction database** ($\\mathcal{D}$): a list of transactions.\n",
    "\n",
    "For example:\n",
    "\n",
    "| **Basket ID** | **Items**                    |\n",
    "|:--------------:|:-----------------------------|\n",
    "| 1 | {milk, bread, salami} |\n",
    "| 2 | {beer, diapers} |\n",
    "| 3 | {beer, wurst} |\n",
    "| 4 | {beer, baby food, diapers} |\n",
    "| 5 | {diapers, coke, bread} |\n",
    "\n",
    "### Support\n",
    "- **Frequency:** $\\sigma(\\mathcal{I})=\\{j \\mid \\mathcal{T}_j \\supseteq \\mathcal{I}\\}$: basket ids ($j$) of transactions containing all items in $\\mathcal{I}$.\n",
    "- **Support:** $supp(X) = \\frac{|\\sigma(\\mathcal{I})|}{|\\mathcal{D}|}$.\n",
    "Support defines the frequency of an itemset.\n",
    "\n",
    "### Association Rules\n",
    "A rule has the form `X → Y` where `X` and `Y` are disjoint itemsets.\n",
    "\n",
    "- **Confidence**: $conf(X \\rightarrow Y) = supp(X \\cup Y) / supp(X)$. How reliable the rule is: given the left-hand side, how often does the right-hand side also happen?\n",
    "> If 100 customers buy bread, and 60 of those also buy butter, then the confidence of the rule\n",
    "bread → butter is 0.6 (60%).\n",
    "- **Lift**: $lift(X \\rightarrow Y) = conf(X \\rightarrow Y) / supp(Y)$. How much more often the items occur together than we’d expect by chance.\n",
    "> If 20% of all customers buy butter, but 60% of bread buyers buy butter, then lift = 0.6 / 0.2 = 3.0. This means: bread buyers are 3× more likely to buy butter than a random shopper.\n",
    "- **Interestingness**: $int(X \\rightarrow Y) = conf(X \\rightarrow Y) - supp(Y)$. How useful or surprising a rule is?\n",
    "> A rule like milk → bread might have high support and confidence but low interestingness (too obvious). A rarer rule like wine → cheese might have lower support but higher interestingness if it reveals a meaningful shopping pattern.\n",
    "\n",
    "\n",
    "**Anti-monotonicity (Apriori principle):** If an itemset `X` is frequent, then **all** subsets of `X` are also frequent. Conversely, if a subset is infrequent, any superset is infrequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d2780fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_frozenset(txn):\n",
    "    '''Normalize a transaction/list/tuple to a frozenset for hashing.'''\n",
    "    return frozenset(txn)\n",
    "\n",
    "def support_counts(transactions):\n",
    "    '''Return a Counter mapping frozenset(itemset)->count for all singletons.'''\n",
    "    c = Counter()\n",
    "    for t in transactions:\n",
    "        fs = to_frozenset(t)\n",
    "        for item in fs:\n",
    "            c[frozenset([item])] += 1\n",
    "    return c\n",
    "\n",
    "def count_support_of_itemsets(transactions, candidates):\n",
    "    '''Count support for each candidate itemset over transactions.'''\n",
    "    counts = Counter()\n",
    "    for t in map(to_frozenset, transactions):\n",
    "        for c in candidates:\n",
    "            if c.issubset(t):\n",
    "                counts[c] += 1\n",
    "    return counts\n",
    "\n",
    "def support_fraction(count, num_transactions):\n",
    "    return count / max(1, num_transactions)\n",
    "\n",
    "def rule_metrics(supp_xy, supp_x, supp_y, n):\n",
    "    '''Compute standard rule metrics given supports as counts and DB size n.'''\n",
    "    s_xy = \n",
    "    s_x  = \n",
    "    s_y  = \n",
    "    conf = \n",
    "    lift = \n",
    "    intr = \n",
    "    return {\"support\": s_xy, \"confidence\": conf, \"lift\": lift, \"interestingness\": intr}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b483c91",
   "metadata": {},
   "source": [
    "### Sample Transaction Dataset\n",
    "\n",
    "We'll use a tiny \"market basket\" sample to illustrate calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27f10a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 transactions.\n",
      " 0: ['bread', 'eggs', 'milk']\n",
      " 1: ['bread', 'butter']\n",
      " 2: ['bread', 'butter', 'eggs', 'milk']\n",
      " 3: ['beer', 'bread']\n",
      " 4: ['beer', 'bread', 'diapers', 'milk']\n",
      " 5: ['bread', 'butter', 'diapers']\n",
      " 6: ['beer', 'cola', 'diapers', 'milk']\n",
      " 7: ['beer', 'bread', 'diapers', 'milk']\n"
     ]
    }
   ],
   "source": [
    "# Toy Transactions\n",
    "transactions = [\n",
    "    {\"milk\",\"bread\",\"eggs\"},\n",
    "    {\"bread\",\"butter\"},\n",
    "    {\"milk\",\"bread\",\"butter\",\"eggs\"},\n",
    "    {\"beer\",\"bread\"},\n",
    "    {\"milk\",\"diapers\",\"beer\",\"bread\"},\n",
    "    {\"diapers\",\"bread\",\"butter\"},\n",
    "    {\"milk\",\"diapers\",\"beer\",\"cola\"},\n",
    "    {\"bread\",\"milk\",\"diapers\",\"beer\"},\n",
    "]\n",
    "n_txn = len(transactions)\n",
    "print(f\"Loaded {n_txn} transactions.\")\n",
    "for i, t in enumerate(transactions):\n",
    "    print(f\"{i:>2}: {sorted(t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79bc35c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singleton supports:\n",
      " 1. {bread}                                  support=0.875 (count=7)\n",
      " 2. {milk}                                   support=0.625 (count=5)\n",
      " 3. {beer}                                   support=0.500 (count=4)\n",
      " 4. {diapers}                                support=0.500 (count=4)\n",
      " 5. {butter}                                 support=0.375 (count=3)\n",
      " 6. {eggs}                                   support=0.250 (count=2)\n",
      " 7. {cola}                                   support=0.125 (count=1)\n",
      "\n",
      "Rule {milk} → {bread} metrics:\n",
      "  support: 0.500\n",
      "  confidence: 0.800\n",
      "  lift: 0.914\n",
      "  interestingness: -0.075\n"
     ]
    }
   ],
   "source": [
    "# Compute Basic Supports and Example Rules\n",
    "sing_counts = support_counts(transactions)\n",
    "print(\"Singleton supports:\")\n",
    "print_itemsets(sing_counts, n_txn)\n",
    "\n",
    "# Example: compute support and rule metrics for {milk} -> {bread}\n",
    "X = frozenset([\"milk\"]); Y = frozenset([\"bread\"])\n",
    "supp_x  = sing_counts[X]\n",
    "supp_y  = sing_counts[Y]\n",
    "supp_xy = count_support_of_itemsets(transactions, [X|Y])[X|Y]\n",
    "\n",
    "metrics = rule_metrics(supp_xy, supp_x, supp_y, n_txn)\n",
    "print(f\"\\nRule {format_itemset(X)} → {format_itemset(Y)} metrics:\")\n",
    "for k,v in metrics.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f959ce",
   "metadata": {},
   "source": [
    "## Maximal, Closed, and Closed Frequent Itemsets\n",
    "\n",
    "Let $\\mathcal{F}$ be the set of all frequent itemsets for a given $minsup$ (minimum support threshold).\n",
    "\n",
    "- **Maximal Frequent Itemset (MFI)**: a frequent itemset with **no frequent superset**.  \n",
    "  Formally, $X$ in $F$ is maximal if there is no $Y$ in $F$ such that $X$ is a proper subset of $Y$.\n",
    "\n",
    "- **Closed Itemset (CI)**: an itemset $X$ is *closed* if **no proper superset** of $X$ has the **same support count** as $X$.\n",
    "\n",
    "- **Closed Frequent Itemset (CFI)**: an itemset that is both **frequent** and **closed**.\n",
    "\n",
    "**Why they matter**\n",
    "- MFIs compress the result set by keeping only the \"largest\" frequent patterns.\n",
    "- CFIs preserve exact support of all frequent itemsets (lossless compression), enabling rule generation with fewer patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e26ab",
   "metadata": {},
   "source": [
    "## Apriori Algorithm\n",
    "\n",
    "**Idea:** Use the anti-monotone property of support to prune the search space.\n",
    "If an itemset of size $k-1$ is not frequent, then any $k$-itemset (itemset with $k$ element) containing it cannot be frequent.\n",
    "\n",
    "### Pseudocode (high-level)\n",
    "1. Count support of all singletons → $F_1 = \\{ \\text{frequent 1-itemsets} \\}$.\n",
    "2. For $k = 2, 3, \\dots$\n",
    "   1. **Join:** form candidate $k$-itemsets $C_k$ by joining pairs in $F_{k-1}$ that share $k-2$ items.  \n",
    "   2. **Prune:** remove any $c \\in C_k$ if it has a $(k-1)$-subset not in $F_{k-1}$.  \n",
    "   3. Count supports of $C_k$ with one scan of DB → $F_k = \\{ c \\in C_k \\mid supp(c) \\geq minsup \\}$.  \n",
    "   4. Stop when $F_k$ is empty.\n",
    "4. All frequent itemsets $F = \\bigcup_{k=1} F_k$.\n",
    "\n",
    "**Pros**: Simple, interpretable, good when minsup is not too low and dimensionality moderate.  \n",
    "**Cons**: Many candidate generations + multiple scans; scales poorly for low minsup / high dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f2a8b",
   "metadata": {},
   "source": [
    "## Park–Chen–Yu (PCY) Algorithm (Frequent Pairs)\n",
    "\n",
    "**Goal:** Reduce memory for candidate itemsets by hashing pairs during the first pass.\n",
    "\n",
    "### Key idea (two-pass)\n",
    "1. **Pass 1** (count $(n-1)$-itemsets + hash $n$-itemset):\n",
    "   - Count single items to find frequent $(n-1)$-itemsets $L_{n-1}$.\n",
    "   - For every transaction, hash each unordered n-itemsets into one of $B$ buckets and increment that bucket's count.\n",
    "   - After pass 1, mark buckets with count $\\geq minsup$ as **frequent** (bitmap).\n",
    "2. **Pass 2** (count candidate pairs only):\n",
    "   - Candidate $n$-itemset is considered **only if**:\n",
    "     1. all subsets are frequent in $L_{n-1}$, and\n",
    "     2. $hash(n\\text{-itemset})$ hits a **frequent bucket**.\n",
    "   - Count these candidates and keep those reaching $minsup$."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example\n",
    "\n",
    "| **Basket ID** | **Items**    |\n",
    "|:-------------:|:-------------|\n",
    "|       1       | {2, 4}       |\n",
    "|       2       | {1, 3, 5}    |\n",
    "|       3       | {1, 3, 4}    |\n",
    "|       4       | {1, 2, 5}    |\n",
    "|       5       | {1, 2, 3, 5} |\n",
    "|       6       | {1, 5}       |\n",
    "|       7       | {2, 4}       |\n",
    "|       8       | {1, 3}       |\n",
    "|       9       | {2, 3}       |\n",
    "\n",
    "$Min.Sup = 4$\n",
    "\n",
    "__Iteration 1.:__\n",
    "\n",
    "Candidate set:\n",
    "\n",
    "$C_1$ := $\\{1, 2, 3, 4, 5\\}$\n",
    "\n",
    "__Pass 1__\n",
    "\n",
    "Counts:\n",
    "\n",
    "$L_1$ := $\\{1: 6, 2: 5, 3: 5, 5: 4\\}$\n",
    "\n",
    "Hash function:\n",
    "\n",
    "$h(x, y) = [3*(x + y)]\\,\\text{mod}\\,5$\n",
    "\n",
    "Pairs (from frequent singles) to hash from transactional database:\n",
    "\n",
    "$T_2 = (1, 3), (1, 5), (3, 5)$\n",
    "\n",
    "$T_3 = (1, 3)$\n",
    "\n",
    "$T_4 = (1, 2), (1, 5), (2, 5)$\n",
    "\n",
    "$T_5 = (1, 2), (1, 3), (1, 5), (2, 3), (2, 5), (3, 5)$\n",
    "\n",
    "$T_6 = (1, 5)$\n",
    "\n",
    "$T_8 = (1, 3)$\n",
    "\n",
    "$T_9 = (2, 3)$\n",
    "\n",
    "Final hash buckets:\n",
    "\n",
    "$B_1$ := $\\{0: 2, 1: 2, 2: 4, 3: 4, 4: 4\\}$\n",
    "\n",
    "__Pass 2__\n",
    "\n",
    "We are going to generate item-pairs of frequent items:\n",
    "- (1, 2) -> h(1, 2) = 4 (frequent bucket) -> candidate\n",
    "- (1, 3) -> h(1, 3) = 2 (frequent bucket) -> candidate\n",
    "- (1, 5) -> h(1, 5) = 3 (frequent bucket) -> candidate\n",
    "- (2, 3) -> h(2, 3) = 0\n",
    "- (2, 5) -> h(2, 5) = 1\n",
    "- (3, 5) -> h(3, 5) = 4 (frequent bucket) -> candidate\n",
    "\n",
    "Candidate set:\n",
    "$C_2$ = {(1, 2), (1, 3), (1, 5), (3, 5)}"
   ],
   "id": "3aa09601ef4bb923"
  },
  {
   "cell_type": "markdown",
   "id": "aa68d104",
   "metadata": {},
   "source": [
    "## FP–Growth: Frequent Pattern Growth\n",
    "\n",
    "**Idea:** Avoid generating candidate sets explicitly. Build a compressed prefix-tree (**FP-tree**) of the database using **frequency-sorted** items, then mine frequent patterns via **conditional pattern bases** and **conditional FP-trees** recursively.\n",
    "\n",
    "### Steps\n",
    "1. **Single pass to count item supports**; discard infrequent items.\n",
    "2. **Order items** in each transaction by descending global frequency (and break ties deterministically, e.g., lexicographically), then **insert** into an FP-tree (a compact prefix tree with counts).\n",
    "3. **Mine recursively**:\n",
    "   - For each item $i$ in the header table (from least frequent to most), build its **conditional pattern base** (multiset of prefix paths ending at $i$).\n",
    "   - Build a **conditional FP-tree** from that base and mine recursively to get all frequent itemsets that include $i$.\n",
    "\n",
    "**Pros**: Fewer passes over DB, avoids explosive candidate generation, works well for dense datasets.  \n",
    "**Cons**: Tree may still blow up on datasets with low overlap."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example\n",
    "\n",
    "__1. First pass: frequent 1-items__\n",
    "\n",
    "Counts (as before in the case of PCY):\n",
    "\n",
    "1 → 6\n",
    "\n",
    "2 → 5\n",
    "\n",
    "3 → 5\n",
    "\n",
    "4 → 3\n",
    "\n",
    "5 → 4\n",
    "\n",
    "Frequent items (support ≥ 4):\n",
    "\n",
    "L1={1,2,3,5}\n",
    "\n",
    "We drop item 4 from all transactions.\n",
    "\n",
    "__2. Reorder transactions by frequency order__\n",
    "\n",
    "Global frequency order (tie-broken by item ID):\n",
    "\n",
    "1 (6), 2 (5), 3 (5), 5 (4)\n",
    "→ ordering: 1 > 2 > 3 > 5\n",
    "\n",
    "Now rewrite each basket keeping only {1,2,3,5} and sorting items by this order:\n",
    "\n",
    "T1: {2,4} → {2}\n",
    "\n",
    "T2: {1,3,5} → {1,3,5}\n",
    "\n",
    "T3: {1,3,4} → {1,3}\n",
    "\n",
    "T4: {1,2,5} → {1,2,5}\n",
    "\n",
    "T5: {1,2,3,5} → {1,2,3,5}\n",
    "\n",
    "T6: {1,5} → {1,5}\n",
    "\n",
    "T7: {2,4} → {2}\n",
    "\n",
    "T8: {1,3} → {1,3}\n",
    "\n",
    "T9: {2,3} → {2,3}\n",
    "\n",
    "These ordered “frequent-only” transactions will be inserted into the FP-tree.\n",
    "\n",
    "__3. Build the FP-tree__\n",
    "\n",
    "We start with an empty root (∅).\n",
    "\n",
    "I’ll show the tree growth transaction by transaction.\n",
    "\n",
    "Insert T1: [2]\n",
    "\n",
    "Tree:\n",
    "\n",
    "```\n",
    "root\n",
    "└─ 2:1\n",
    "```\n",
    "\n",
    "Insert T2: [1,3,5]\n",
    "\n",
    "```\n",
    "root\n",
    "├─ 2:1\n",
    "└─ 1:1\n",
    "   └─ 3:1\n",
    "      └─ 5:1\n",
    "```\n",
    "\n",
    "Insert T3: [1,3]\n",
    "\n",
    "root → 1 exists (1:1 → 1:2), 1 → 3 exists (3:1 → 3:2):\n",
    "\n",
    "```\n",
    "root\n",
    "├─ 2:1\n",
    "└─ 1:2\n",
    "   └─ 3:2\n",
    "      └─ 5:1\n",
    "```\n",
    "\n",
    "Insert T4: [1,2,5]\n",
    "\n",
    "root → 1 (2 → 3)\n",
    "under 1, child 2 doesn’t exist yet → create 2:1\n",
    "under (1,2), add 5:1:\n",
    "\n",
    "```\n",
    "root\n",
    "├─ 2:1\n",
    "└─ 1:3\n",
    "   ├─ 3:2\n",
    "   │  └─ 5:1\n",
    "   └─ 2:1\n",
    "      └─ 5:1\n",
    "```\n",
    "\n",
    "Insert T5: [1,2,3,5]\n",
    "\n",
    "root → 1 (3 → 4)\n",
    "1 → 2 exists (1 → 2), so 2:1 → 2:2\n",
    "under (1,2), child 3 does not exist yet → add 3:1\n",
    "under (1,2,3), add 5:1:\n",
    "\n",
    "```\n",
    "root\n",
    "├─ 2:1\n",
    "└─ 1:4\n",
    "   ├─ 3:2\n",
    "   │  └─ 5:1\n",
    "   └─ 2:2\n",
    "      ├─ 5:1\n",
    "      └─ 3:1\n",
    "         └─ 5:1\n",
    "```\n",
    "\n",
    "Insert T6: [1,5]\n",
    "\n",
    "root → 1 (4 → 5)\n",
    "under 1, no direct child 5 yet, so create 5:1:\n",
    "\n",
    "```\n",
    "root\n",
    "├─ 2:1\n",
    "└─ 1:5\n",
    "   ├─ 3:2\n",
    "   │  └─ 5:1\n",
    "   ├─ 2:2\n",
    "   │  ├─ 5:1\n",
    "   │  └─ 3:1\n",
    "   │     └─ 5:1\n",
    "   └─ 5:1\n",
    "```\n",
    "\n",
    "Insert T7: [2]\n",
    "\n",
    "root → 2 exists (1 → 2):\n",
    "\n",
    "```\n",
    "root\n",
    "├─ 2:2\n",
    "└─ 1:5\n",
    "   ├─ 3:2\n",
    "   │  └─ 5:1\n",
    "   ├─ 2:2\n",
    "   │  ├─ 5:1\n",
    "   │  └─ 3:1\n",
    "   │     └─ 5:1\n",
    "   └─ 5:1\n",
    "```\n",
    "\n",
    "Insert T8: [1,3]\n",
    "\n",
    "root → 1 (5 → 6)\n",
    "under 1, child 3 exists (2 → 3):\n",
    "\n",
    "```\n",
    "root\n",
    "├─ 2:2\n",
    "└─ 1:6\n",
    "   ├─ 3:3\n",
    "   │  └─ 5:1\n",
    "   ├─ 2:2\n",
    "   │  ├─ 5:1\n",
    "   │  └─ 3:1\n",
    "   │     └─ 5:1\n",
    "   └─ 5:1\n",
    "```\n",
    "\n",
    "Insert T9: [2,3]\n",
    "\n",
    "root → 2 (2 → 3)\n",
    "under 2, child 3 doesn’t exist → add 3:1:\n",
    "\n",
    "Final FP-tree:\n",
    "\n",
    "```\n",
    "root\n",
    "├─ 2:3\n",
    "│  └─ 3: 1\n",
    "└─ 1:6\n",
    "   ├─ 3:3\n",
    "   │  └─ 5:1\n",
    "   ├─ 2:2\n",
    "   │  ├─ 5:1\n",
    "   │  └─ 3:1\n",
    "   │     └─ 5:1\n",
    "   └─ 5:1\n",
    "```\n",
    "\n",
    "We also have a header table with node-links for each item (1,2,3,5), but we’ll just use it conceptually when we traverse “all 5-nodes”, “all 3-nodes”, etc.\n",
    "\n",
    "__4. Mining the FP-tree (FP-Growth)__\n",
    "\n",
    "We mine items in increasing frequency order (i.e., bottom-up):\n",
    "- Global order: 1(6), 2(5), 3(5), 5(4); where the count can be found between the parenthesis.\n",
    "- Mining order (least frequent first): 5, 3, 2, 1\n",
    "\n",
    "For each item $i$:\n",
    "- Collect its conditional pattern base (prefix paths leading to $i$).\n",
    "- Build its conditional FP-tree.\n",
    "- Generate all frequent patterns that end with $i$.\n",
    "\n",
    "__4.1. Mining item 5__\n",
    "\n",
    "Find all nodes labeled 5 in the FP-tree:\n",
    "- Path root → 1 → 3 → 5:1 → prefix: $[1,3]$, count 1\n",
    "- Path root → 1 → 2 → 5:1 → prefix: $[1,2]$, count 1\n",
    "- Path root → 1 → 2 → 3 → 5:1 → prefix: $[1,2,3]$, count 1\n",
    "- Path root → 1 → 5:1 → prefix: $[1]$, count 1\n",
    "\n",
    "So the conditional pattern base for 5 is:\n",
    "- $[1,3]$ : 1\n",
    "- $[1,2]$ : 1\n",
    "- $[1,2,3]$ : 1\n",
    "- $[1]$ : 1\n",
    "\n",
    "Now count items in this base:\n",
    "- From [1,3]: 1(+1), 3(+1)\n",
    "- From [1,2]: 1(+1), 2(+1)\n",
    "- From [1,2,3]: 1(+1), 2(+1), 3(+1)\n",
    "- From [1]: 1(+1)\n",
    "\n",
    "Totals:\n",
    "- 1 → 4\n",
    "- 2 → 2\n",
    "- 3 → 2\n",
    "\n",
    "Keep only items with support ≥ 4 → only item 1 survives.\n",
    "So the conditional FP-tree for item 5 is just:\n",
    "\n",
    "```\n",
    "root_5\n",
    "└─ 1\n",
    "```\n",
    "\n",
    "From this tree we get frequent patterns including 5:\n",
    "- {5} with support 4 (we know from L₁)\n",
    "- {1,5} with support 4\n",
    "({2,5} and {3,5} have conditional support 2 < 4, so they’re not frequent.)\n",
    "\n",
    "__4.2. Mining item 3__\n",
    "\n",
    "Find all nodes labeled 3 in the main FP-tree:\n",
    "- root → 1 → 3:3 → prefix [1], count 3\n",
    "- root → 1 → 2 → 3:1 → prefix [1,2], count 1\n",
    "- root → 2 → 3:1 → prefix [2], count 1\n",
    "\n",
    "So the conditional pattern base for 3:\n",
    "- [1] : 3\n",
    "- [1,2] : 1\n",
    "- [2] : 1\n",
    "\n",
    "Item counts in this base:\n",
    "- [1]: 1(+3) → 1:3\n",
    "- [1,2]: 1(+1), 2(+1) → 1:4, 2:1\n",
    "- [2]: 2(+1) → 2:2\n",
    "\n",
    "Totals:\n",
    "- 1 → 4\n",
    "- 2 → 2\n",
    "\n",
    "Frequent (≥ 4): only item 1.\n",
    "\n",
    "Conditional FP-tree for 3:\n",
    "\n",
    "```\n",
    "root_3\n",
    "└─ 1\n",
    "```\n",
    "\n",
    "Frequent patterns involving 3:\n",
    "- {3} with support 5\n",
    "- {1,3} with support 4\n",
    "({2,3} has support 3 < 4, so not frequent.)\n",
    "\n",
    "__4.3. Mining item 2__\n",
    "\n",
    "Nodes labeled 2:\n",
    "- root → 2:3 → prefix [] (empty), count 3 → contributes nothing to conditional base (no items before it).\n",
    "- root → 1 → 2:2 → prefix [1], count 2\n",
    "\n",
    "So conditional pattern base for 2:\n",
    "- [1] : 2\n",
    "\n",
    "Counts:\n",
    "- 1 → 2\n",
    "No item reaches support 4 here → no multi-item patterns with 2.\n",
    "\n",
    "Frequent patterns:\n",
    "- {2} with support 5\n",
    "(no {1,2}, since support is 2)\n",
    "\n",
    "__4.4. Mining item 1__\n",
    "\n",
    "Item 1 is the most frequent and always at the top of paths; there’s no non-empty prefix before 1, so you only get:\n",
    "- {1} with support 6\n",
    "\n",
    "__5. Collect all frequent patterns__\n",
    "\n",
    "From the mining steps:\n",
    "\n",
    "**1-itemsets**\n",
    "- {1} : 6\n",
    "- {2} : 5\n",
    "- {3} : 5\n",
    "- {5} : 4\n",
    "\n",
    "**2-itemsets**\n",
    "- {1,3} : 4\n",
    "- {1,5} : 4\n",
    "\n",
    "No 3-itemset survives:\n",
    "For example, {1,3,5} only appears in baskets 2 and 5 → support 2 < 4.\n",
    "\n",
    "So the final frequent itemsets (Min.Sup = 4) are:\n",
    "\n",
    "Single items:\n",
    "- {1},{2},{3},{5}\n",
    "\n",
    "Pairs:\n",
    "- {1,3},{1,5}"
   ],
   "id": "3abac1035789e737"
  },
  {
   "cell_type": "markdown",
   "id": "75cf991e-3b35-4635-ae9d-8f93c1bb7710",
   "metadata": {},
   "source": [
    "# Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ef064",
   "metadata": {},
   "source": [
    "## When to Use What? (Apriori vs. PCY vs. FP–Growth)\n",
    "\n",
    "- **Apriori**:\n",
    "    - small/medium problems; uses many DB passes\n",
    "    - and can explode with low support.\n",
    "- **PCY**:\n",
    "    - uses hashing to prune candidate pairs drastically.\n",
    "- **FP–Growth**:\n",
    "    - often fastest on dense datasets or low $minsup$\n",
    "    - avoids candidate generation via compression & recursive mining.\n",
    "\n",
    "**Heuristics**\n",
    "- If you only need **frequent pairs** and memory is tight → **PCY**.\n",
    "- If you need **all frequent itemsets** and dataset is **dense** → **FP–Growth**.\n",
    "- If you want a simple, transparent baseline or **higher minsup** → **Apriori**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
