{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e608d7e",
   "metadata": {},
   "source": [
    "# Frequent Itemset Mining\n",
    "\n",
    "In many domains -- from retail to web analytics, from bioinformatics to cybersecurity -- we are surrounded by discrete event data: logs of things that co-occur, purchases that happen together, or features that frequently appear in patterns. The **goal of frequent pattern mining** (or frequent data mining) **is to uncover regularities, co-occurrences, and associations hidden in these large transactional datasets**.\n",
    "\n",
    "\n",
    "**Market Basket Analysis**\n",
    "- Discover that customers who buy milk and bread often also buy butter.\n",
    "- This insight drives store layout, cross-selling, and recommendation systems.\n",
    "\n",
    "**Web & Clickstream Mining**\n",
    "- Identify frequent sequences of pages or actions, e.g., home → search → checkout.\n",
    "- Helps optimize navigation design or ad placement.\n",
    "\n",
    "**Healthcare & Bioinformatics**\n",
    "- Find frequent combinations of symptoms or genes associated with conditions.\n",
    "- Enables diagnostic rule discovery and biomarker identification.\n",
    "\n",
    "**Network Security**\n",
    "- Detect frequent combinations of log events or packet signatures indicative of an attack pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b306667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations, chain\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def powerset(iterable):\n",
    "    '''Return all non-empty proper subsets of an iterable as frozensets.'''\n",
    "    s = list(iterable)\n",
    "    for r in range(1, len(s)):\n",
    "        for comb in combinations(s, r):\n",
    "            yield frozenset(comb)\n",
    "\n",
    "def format_itemset(iset):\n",
    "    return \"{\" + \", \".join(sorted(map(str, iset))) + \"}\"\n",
    "\n",
    "def print_itemsets(freq_dict, num_transactions, max_items=20):\n",
    "    '''Pretty-print up to `max_items` itemsets with support as fraction and count.'''\n",
    "    items = list(freq_dict.items())\n",
    "    items.sort(key=lambda kv: (-kv[1], sorted(list(kv[0]))))\n",
    "    for i, (iset, cnt) in enumerate(items[:max_items], start=1):\n",
    "        sup = cnt / max(1, num_transactions)\n",
    "        print(f\"{i:>2}. {format_itemset(iset):<40} support={sup:.3f} (count={cnt})\")\n",
    "\n",
    "def without(iterable, item):\n",
    "    '''Return iterable minus a single item.'''\n",
    "    return [x for x in iterable if x != item]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b13cc9",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "### Items & Transactions\n",
    "- **Item**: an atomic symbol (e.g., `\"milk\"`, `\"bread\"`, `\"diapers\"`).\n",
    "- **Itemset** ($\\mathcal{I}$): a set of items, e.g., `{milk, bread}`.\n",
    "- **Transaction** ($\\mathcal{T}$): a set (or list) of items e.g., items purchased/observed together.\n",
    "- **Transaction database** ($\\mathcal{D}$): a list of transactions.\n",
    "\n",
    "For example:\n",
    "\n",
    "| **Basket ID** | **Items**                    |\n",
    "|:--------------:|:-----------------------------|\n",
    "| 1 | {milk, bread, salami} |\n",
    "| 2 | {beer, diapers} |\n",
    "| 3 | {beer, wurst} |\n",
    "| 4 | {beer, baby food, diapers} |\n",
    "| 5 | {diapers, coke, bread} |\n",
    "\n",
    "### Support\n",
    "- **Frequency:** $\\sigma(\\mathcal{I})=\\{j \\mid \\mathcal{T}_j \\supseteq \\mathcal{I}\\}$: basket ids ($j$) of transactions containing all items in $\\mathcal{I}$.\n",
    "- **Support:** $supp(X) = \\frac{|\\sigma(\\mathcal{I})|}{|\\mathcal{D}|}$.\n",
    "Support defines the frequency of an itemset.\n",
    "\n",
    "### Association Rules\n",
    "A rule has the form `X → Y` where `X` and `Y` are disjoint itemsets.\n",
    "\n",
    "- **Confidence**: $conf(X \\rightarrow Y) = supp(X \\cup Y) / supp(X)$. How reliable the rule is: given the left-hand side, how often does the right-hand side also happen?\n",
    "> If 100 customers buy bread, and 60 of those also buy butter, then the confidence of the rule\n",
    "bread → butter is 0.6 (60%).\n",
    "- **Lift**: $lift(X \\rightarrow Y) = conf(X \\rightarrow Y) / supp(Y)$. How much more often the items occur together than we’d expect by chance.\n",
    "> If 20% of all customers buy butter, but 60% of bread buyers buy butter, then lift = 0.6 / 0.2 = 3.0. This means: bread buyers are 3× more likely to buy butter than a random shopper.\n",
    "- **Interestingness**: $int(X \\rightarrow Y) = conf(X \\rightarrow Y) - supp(Y)$. How useful or surprising a rule is?\n",
    "> A rule like milk → bread might have high support and confidence but low interestingness (too obvious). A rarer rule like wine → cheese might have lower support but higher interestingness if it reveals a meaningful shopping pattern.\n",
    "\n",
    "\n",
    "**Anti-monotonicity (Apriori principle):** If an itemset `X` is frequent, then **all** subsets of `X` are also frequent. Conversely, if a subset is infrequent, any superset is infrequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d2780fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_frozenset(txn):\n",
    "    '''Normalize a transaction/list/tuple to a frozenset for hashing.'''\n",
    "    return frozenset(txn)\n",
    "\n",
    "def support_counts(transactions):\n",
    "    '''Return a Counter mapping frozenset(itemset)->count for all singletons.'''\n",
    "    c = Counter()\n",
    "    for t in transactions:\n",
    "        fs = to_frozenset(t)\n",
    "        for item in fs:\n",
    "            c[frozenset([item])] += 1\n",
    "    return c\n",
    "\n",
    "def count_support_of_itemsets(transactions, candidates):\n",
    "    '''Count support for each candidate itemset over transactions.'''\n",
    "    counts = Counter()\n",
    "    for t in map(to_frozenset, transactions):\n",
    "        for c in candidates:\n",
    "            if c.issubset(t):\n",
    "                counts[c] += 1\n",
    "    return counts\n",
    "\n",
    "def support_fraction(count, num_transactions):\n",
    "    return count / max(1, num_transactions)\n",
    "\n",
    "def rule_metrics(supp_xy, supp_x, supp_y, n):\n",
    "    '''Compute standard rule metrics given supports as counts and DB size n.'''\n",
    "    s_xy = \n",
    "    s_x  = \n",
    "    s_y  = \n",
    "    conf = \n",
    "    lift = \n",
    "    intr = \n",
    "    return {\"support\": s_xy, \"confidence\": conf, \"lift\": lift, \"interestingness\": intr}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b483c91",
   "metadata": {},
   "source": [
    "### Sample Transaction Dataset\n",
    "\n",
    "We'll use a tiny \"market basket\" sample to illustrate calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27f10a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 transactions.\n",
      " 0: ['bread', 'eggs', 'milk']\n",
      " 1: ['bread', 'butter']\n",
      " 2: ['bread', 'butter', 'eggs', 'milk']\n",
      " 3: ['beer', 'bread']\n",
      " 4: ['beer', 'bread', 'diapers', 'milk']\n",
      " 5: ['bread', 'butter', 'diapers']\n",
      " 6: ['beer', 'cola', 'diapers', 'milk']\n",
      " 7: ['beer', 'bread', 'diapers', 'milk']\n"
     ]
    }
   ],
   "source": [
    "# Toy Transactions\n",
    "transactions = [\n",
    "    {\"milk\",\"bread\",\"eggs\"},\n",
    "    {\"bread\",\"butter\"},\n",
    "    {\"milk\",\"bread\",\"butter\",\"eggs\"},\n",
    "    {\"beer\",\"bread\"},\n",
    "    {\"milk\",\"diapers\",\"beer\",\"bread\"},\n",
    "    {\"diapers\",\"bread\",\"butter\"},\n",
    "    {\"milk\",\"diapers\",\"beer\",\"cola\"},\n",
    "    {\"bread\",\"milk\",\"diapers\",\"beer\"},\n",
    "]\n",
    "n_txn = len(transactions)\n",
    "print(f\"Loaded {n_txn} transactions.\")\n",
    "for i, t in enumerate(transactions):\n",
    "    print(f\"{i:>2}: {sorted(t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79bc35c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singleton supports:\n",
      " 1. {bread}                                  support=0.875 (count=7)\n",
      " 2. {milk}                                   support=0.625 (count=5)\n",
      " 3. {beer}                                   support=0.500 (count=4)\n",
      " 4. {diapers}                                support=0.500 (count=4)\n",
      " 5. {butter}                                 support=0.375 (count=3)\n",
      " 6. {eggs}                                   support=0.250 (count=2)\n",
      " 7. {cola}                                   support=0.125 (count=1)\n",
      "\n",
      "Rule {milk} → {bread} metrics:\n",
      "  support: 0.500\n",
      "  confidence: 0.800\n",
      "  lift: 0.914\n",
      "  interestingness: -0.075\n"
     ]
    }
   ],
   "source": [
    "# Compute Basic Supports and Example Rules\n",
    "sing_counts = support_counts(transactions)\n",
    "print(\"Singleton supports:\")\n",
    "print_itemsets(sing_counts, n_txn)\n",
    "\n",
    "# Example: compute support and rule metrics for {milk} -> {bread}\n",
    "X = frozenset([\"milk\"]); Y = frozenset([\"bread\"])\n",
    "supp_x  = sing_counts[X]\n",
    "supp_y  = sing_counts[Y]\n",
    "supp_xy = count_support_of_itemsets(transactions, [X|Y])[X|Y]\n",
    "\n",
    "metrics = rule_metrics(supp_xy, supp_x, supp_y, n_txn)\n",
    "print(f\"\\nRule {format_itemset(X)} → {format_itemset(Y)} metrics:\")\n",
    "for k,v in metrics.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f959ce",
   "metadata": {},
   "source": [
    "## Maximal, Closed, and Closed Frequent Itemsets\n",
    "\n",
    "Let $\\mathcal{F}$ be the set of all frequent itemsets for a given $minsup$ (minimum support threshold).\n",
    "\n",
    "- **Maximal Frequent Itemset (MFI)**: a frequent itemset with **no frequent superset**.  \n",
    "  Formally, $X$ in $F$ is maximal if there is no $Y$ in $F$ such that $X$ is a proper subset of $Y$.\n",
    "\n",
    "- **Closed Itemset (CI)**: an itemset $X$ is *closed* if **no proper superset** of $X$ has the **same support count** as $X$.\n",
    "\n",
    "- **Closed Frequent Itemset (CFI)**: an itemset that is both **frequent** and **closed**.\n",
    "\n",
    "**Why they matter**\n",
    "- MFIs compress the result set by keeping only the \"largest\" frequent patterns.\n",
    "- CFIs preserve exact support of all frequent itemsets (lossless compression), enabling rule generation with fewer patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e26ab",
   "metadata": {},
   "source": [
    "## Apriori Algorithm\n",
    "\n",
    "**Idea:** Use the anti-monotone property of support to prune the search space.\n",
    "If an itemset of size $k-1$ is not frequent, then any $k$-itemset (itemset with $k$ element) containing it cannot be frequent.\n",
    "\n",
    "### Pseudocode (high-level)\n",
    "1. Count support of all singletons → $F_1 = \\{ \\text{frequent 1-itemsets} \\}$.\n",
    "2. For $k = 2, 3, \\dots$\n",
    "   1. **Join:** form candidate $k$-itemsets $C_k$ by joining pairs in $F_{k-1}$ that share $k-2$ items.  \n",
    "   2. **Prune:** remove any $c \\in C_k$ if it has a $(k-1)$-subset not in $F_{k-1}$.  \n",
    "   3. Count supports of $C_k$ with one scan of DB → $F_k = \\{ c \\in C_k \\mid supp(c) \\geq minsup \\}$.  \n",
    "   4. Stop when $F_k$ is empty.\n",
    "4. All frequent itemsets $F = \\bigcup_{k=1} F_k$.\n",
    "\n",
    "**Pros**: Simple, interpretable, good when minsup is not too low and dimensionality moderate.  \n",
    "**Cons**: Many candidate generations + multiple scans; scales poorly for low minsup / high dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f2a8b",
   "metadata": {},
   "source": [
    "## Park–Chen–Yu (PCY) Algorithm (Frequent Pairs)\n",
    "\n",
    "**Goal:** Reduce memory for candidate **pairs** (`2`-itemsets) by hashing pairs during the first pass.\n",
    "\n",
    "### Key idea (two-pass)\n",
    "1. **Pass 1** (count singletons + hash pairs):\n",
    "   - Count single items to find frequent 1-itemsets $L_1$.\n",
    "   - For every transaction, hash each unordered pair $(i, j)$ into one of $B$ buckets and increment that bucket's count.\n",
    "   - After pass 1, mark buckets with count $\\geq minsup$ as **frequent** (bitmap).\n",
    "2. **Pass 2** (count candidate pairs only):\n",
    "   - Candidate pair $(i, j)$ is considered **only if**:  \n",
    "     (a) $i$ and $j$ are in $L_1$, and  \n",
    "     (b) $hash(i, j)$ hits a **frequent bucket**.\n",
    "   - Count these candidates and keep those reaching $minsup$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68d104",
   "metadata": {},
   "source": [
    "## FP–Growth: Frequent Pattern Growth\n",
    "\n",
    "**Idea:** Avoid generating candidate sets explicitly. Build a compressed prefix-tree (**FP-tree**) of the database using **frequency-sorted** items, then mine frequent patterns via **conditional pattern bases** and **conditional FP-trees** recursively.\n",
    "\n",
    "### Steps\n",
    "1. **Single pass to count item supports**; discard infrequent items.\n",
    "2. **Order items** in each transaction by descending global frequency (and break ties deterministically, e.g., lexicographically), then **insert** into an FP-tree (a compact prefix tree with counts).\n",
    "3. **Mine recursively**:\n",
    "   - For each item $i$ in the header table (from least frequent to most), build its **conditional pattern base** (multiset of prefix paths ending at $i$).\n",
    "   - Build a **conditional FP-tree** from that base and mine recursively to get all frequent itemsets that include $i$.\n",
    "\n",
    "**Pros**: Fewer passes over DB, avoids explosive candidate generation, works well for dense datasets.  \n",
    "**Cons**: Tree may still blow up on datasets with low overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf991e-3b35-4635-ae9d-8f93c1bb7710",
   "metadata": {},
   "source": [
    "# Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ef064",
   "metadata": {},
   "source": [
    "## When to Use What? (Apriori vs. PCY vs. FP–Growth)\n",
    "\n",
    "- **Apriori**:\n",
    "    - small/medium problems; uses many DB passes\n",
    "    - and can explode with low support.\n",
    "- **PCY**:\n",
    "    - targeted optimization for **pair mining**\n",
    "    - uses hashing to prune candidate pairs drastically.\n",
    "- **FP–Growth**:\n",
    "    - often fastest on dense datasets or low $minsup$\n",
    "    - avoids candidate generation via compression & recursive mining.\n",
    "\n",
    "**Heuristics**\n",
    "- If you only need **frequent pairs** and memory is tight → **PCY**.\n",
    "- If you need **all frequent itemsets** and dataset is **dense** → **FP–Growth**.\n",
    "- If you want a simple, transparent baseline or **higher minsup** → **Apriori**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
